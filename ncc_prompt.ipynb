{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_loader import load_dataset, format_input_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from saved_data/datasets.pkl...\n"
     ]
    }
   ],
   "source": [
    "# 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "# 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "MODEL_NAME = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "\n",
    "\n",
    "RESULTS_DIR = \"./results/prompting/\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# load data\n",
    "train_df, dev_df, test_df = load_dataset()\n",
    "\n",
    "USE_CTX = False # whether to use [CTX] parts of threads (src --> CTX tweets --> parent --> target)\n",
    "VALID_STANCES = ['SUPPORT', 'DENY', 'QUERY', 'COMMENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "PERSONA = \"\"\"You are an expert in rumour stance analysis on twitter.\"\"\"\n",
    "INSTRUCTION = \"\"\"Your task is to classify the stance of the TARGET tweet towards the veracity of the rumour in the SOURCE tweet.\"\"\"\n",
    "INPUT_FORMAT = \"\"\"\\\n",
    "    The input will be provided as a single string containing labeled segments:\n",
    "    \"[SRC] ... [PARENT] ... [TARGET] ...\" (Note: [PARENT] may be omitted if not applicable).\n",
    "    \"\"\"\n",
    "LABEL_DEFNS = \"\"\"\\\n",
    "    Classification Labels:\n",
    "    - SUPPORT: The reply supports the veracity of the source claim\n",
    "    - DENY: The reply denies the veracity of the source claim\n",
    "    - QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
    "    - COMMENT: The reply makes their own comment without a clear contribution to assessing the veracity of the source claim\n",
    "    \"\"\"\n",
    "OUTPUT_FORMAT = \"\"\"\\\n",
    "    Respond with ONLY one word: SUPPORT, DENY, QUERY, or COMMENT\n",
    "    \"\"\"\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "{OUTPUT_FORMAT}\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Text: {thread_context}\n",
    "\n",
    "Task: Classify the stance of [TARGET] towards the veracity of the rumour in [SRC].\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(thread_context):\n",
    "    return USER_PROMPT_TEMPLATE.format(thread_context=thread_context)\n",
    "\n",
    "\n",
    "def build_zero_shot_messages(thread_context):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": build_user_prompt(thread_context)},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_few_shot_messages(thread_context, examples=None):\n",
    "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}]\n",
    "    \n",
    "    if examples:\n",
    "        for ex in examples:\n",
    "            messages.append({\"role\": \"user\", \"content\": build_user_prompt(ex['source'])})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ex['label']})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": build_user_prompt(thread_context)})\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stance_response(text):\n",
    "    text = text.upper().strip()\n",
    "    for label in VALID_STANCES:\n",
    "        if re.search(rf'\\b{label}\\b', text):\n",
    "            return label.lower()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "\n",
    "def generate_response(pipe, messages, max_new_tokens=10): # max n tokens to limit response length\n",
    "    output = pipe(\n",
    "        messages, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False, # deterministic output\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id, # prevent warnings\n",
    "        )\n",
    "    return output[0][\"generated_text\"][-1][\"content\"].strip()\n",
    "\n",
    "\n",
    "# def classify_instance(pipe, input_text, mode=\"zero-shot\", examples=None):\n",
    "#     if mode == \"zero-shot\":\n",
    "#         messages = build_zero_shot_messages(input_text)\n",
    "#     else:\n",
    "#         messages = build_few_shot_messages(input_text, examples)\n",
    "    \n",
    "#     response = generate_response(pipe, messages)\n",
    "#     label = parse_stance_response(response)\n",
    "#     return label, response\n",
    "\n",
    "\n",
    "# --\n",
    "def plot_confusion_matrix(cm, mode, save_path=None):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix ({mode})')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_results(results, model_key, mode):\n",
    "    \"\"\"Save evaluation results to JSON.\"\"\"\n",
    "    output = {\n",
    "        'model': model_key,\n",
    "        'mode': mode,\n",
    "        'macro_f1': results['macro_f1'],\n",
    "        'per_class_f1': results['per_class_f1'],\n",
    "        'predictions': results['predictions'],\n",
    "        'true_labels': results['true_labels'],\n",
    "        'raw_responses': results['raw_responses']\n",
    "    }\n",
    "    \n",
    "    filename = f\"{RESULTS_DIR}{model_key}_{mode}_results.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot examples\n",
    "support_example = train_df[train_df['label_text']=='support'].iloc[0]\n",
    "deny_example = train_df[train_df['label_text']=='deny'].iloc[0]\n",
    "query_example = train_df[train_df['label_text']=='query'].iloc[0]\n",
    "comment_example = train_df[train_df['label_text']=='comment'].iloc[0]\n",
    "\n",
    "\n",
    "def get_few_shot_examples(df, n_per_class=1):\n",
    "    \"\"\"Select stratified random examples for few-shot prompting.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for label in ['support', 'deny', 'query', 'comment']:\n",
    "        class_df = df[df['label_text'] == label]\n",
    "        samples = class_df.sample(n=min(n_per_class, len(class_df)))\n",
    "        \n",
    "        for _, row in samples.iterrows():\n",
    "            examples.append({\n",
    "                'source': format_input_with_context(row, df, use_features=False, use_context=USE_CTX),\n",
    "                'label': label.upper()\n",
    "            })\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    MODEL_NAME, \n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "pipe.tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompting(pipe, df, mode=\"zero-shot\", examples=None, batch_size=4, verbose=True):\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Prepare all messages upfront\n",
    "    all_messages = []\n",
    "    for _, row in df.iterrows():\n",
    "        input_text = format_input_with_context(row, df, use_features=False, use_context=USE_CTX)\n",
    "        if mode == \"zero-shot\":\n",
    "            messages = build_zero_shot_messages(input_text)\n",
    "        else:\n",
    "            messages = build_few_shot_messages(input_text, examples)\n",
    "        all_messages.append(messages)\n",
    "    \n",
    "    # Batched inference using Dataset\n",
    "    dataset = Dataset.from_dict({\"messages\": all_messages})\n",
    "    \n",
    "    raw_responses = []\n",
    "    for i in tqdm(range(0, len(all_messages), batch_size), desc=f\"Evaluating ({mode})\"):\n",
    "        batch = all_messages[i:i+batch_size]\n",
    "        outputs = pipe(batch, max_new_tokens=10, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "        for out in outputs:\n",
    "            raw_responses.append(out[0][\"generated_text\"][-1][\"content\"].strip())\n",
    "    \n",
    "    # Parse responses\n",
    "    predictions = [parse_stance_response(r) for r in raw_responses]\n",
    "\n",
    "    # count number of None (errors), print and replace with comment\n",
    "    num_errors = predictions.count(None)\n",
    "    if num_errors > 0:\n",
    "        print(f\"Warning: {num_errors} errors in predictions. Replacing with 'comment'.\")\n",
    "        predictions = [p if p is not None else 'comment' for p in predictions]\n",
    "\n",
    "    true_labels = df['label_text'].tolist()\n",
    "    \n",
    "    # Metrics\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    per_class_f1 = f1_score(true_labels, predictions, average=None, labels=labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\\nResults ({mode})\\n{'='*60}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"\\nPer-class F1:\")\n",
    "        for lbl, f1 in zip(labels, per_class_f1):\n",
    "            print(f\"  {lbl}: {f1:.4f}\")\n",
    "        print(f\"\\n{classification_report(true_labels, predictions, labels=labels)}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'raw_responses': raw_responses,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': dict(zip(labels, per_class_f1)),\n",
    "        'confusion_matrix': confusion_matrix(true_labels, predictions, labels=labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an expert in rumour stance analysis on twitter.\\nYour task is to classify the stance of the TARGET tweet towards the veracity of the rumour in the SOURCE tweet.\\n    The input will be provided as a single string containing labeled segments:\\n    \"[SRC] ... [PARENT] ... [TARGET] ...\" (Note: [PARENT] may be omitted if not applicable).\\n    \\n    Classification Labels:\\n    - SUPPORT: The reply supports the veracity of the source claim\\n    - DENY: The reply denies the veracity of the source claim\\n    - QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\\n    - COMMENT: The reply makes their own comment without a clear contribution to assessing the veracity of the source claim\\n    \\n    Respond with ONLY one word: SUPPORT, DENY, QUERY, or COMMENT\\n    \\n'}, {'role': 'user', 'content': 'Text: [SRC] Heart goes out to 148 passengers and crew of Germanwings Airbus A320 that has crashed in French Alps , Southern France HTTPURL [TARGET] @USER Hope we will not to hear this kind of news once again !\\n\\nTask: Classify the stance of [TARGET] towards the veracity of the rumour in [SRC].\\n'}]\n",
      "\n",
      "Ground truth: comment\n",
      "Predicted:    support\n",
      "Response:     SUPPORT\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c1f329ad244532bbea17a6d9352455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (zero-shot):   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG ===\n",
      "Total raw_responses: 281\n",
      "First 5 raw responses: ['SUPPORT', 'SUPPORT', 'QUERY', 'SUPPORT', 'QUERY']\n",
      "First 5 predictions: ['support', 'support', 'query', 'support', 'query']\n",
      "First 5 true labels: ['support', 'comment', 'comment', 'comment', 'comment']\n",
      "None count in predictions: 0\n",
      "\n",
      "============================================================\n",
      "Results (zero-shot)\n",
      "============================================================\n",
      "Macro F1: 0.1925\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.3143\n",
      "  deny: 0.0000\n",
      "  query: 0.2481\n",
      "  comment: 0.2075\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.23      0.48      0.31        69\n",
      "        deny       0.00      0.00      0.00        11\n",
      "       query       0.16      0.57      0.25        28\n",
      "     comment       0.56      0.13      0.21       173\n",
      "\n",
      "    accuracy                           0.25       281\n",
      "   macro avg       0.24      0.29      0.19       281\n",
      "weighted avg       0.42      0.25      0.23       281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/nchw73/myjupyterenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home2/nchw73/myjupyterenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home2/nchw73/myjupyterenv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# zero-shot\n",
    "\n",
    "row = dev_df.iloc[1]\n",
    "df = dev_df\n",
    "\n",
    "input_text = format_input_with_context(row, df, use_features=False, use_context=USE_CTX)\n",
    "messages = build_zero_shot_messages(input_text)\n",
    "print(messages)\n",
    "print()\n",
    "\n",
    "# pipeline does this\n",
    "# prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# print(prompt)\n",
    "# print()\n",
    "\n",
    "response = generate_response(pipe, messages)\n",
    "label = parse_stance_response(response)\n",
    "print(f\"Ground truth: {row['label_text']}\")\n",
    "print(f\"Predicted:    {label}\")\n",
    "print(f\"Response:     {response}\")\n",
    "print()\n",
    "\n",
    "zero_results = evaluate_prompting(pipe, dev_df, mode=\"zero-shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 01 Jan 2026\n",
      "\n",
      "You are an expert in rumour stance analysis on twitter.\n",
      "Your task is to classify the stance of the TARGET tweet towards the veracity of the rumour in the SOURCE tweet.\n",
      "    The input will be provided as a single string containing labeled segments:\n",
      "    \"[SRC] ... [PARENT] ... [TARGET] ...\" (Note: [PARENT] may be omitted if not applicable).\n",
      "    \n",
      "    Classification Labels:\n",
      "    - SUPPORT: The reply supports the veracity of the source claim\n",
      "    - DENY: The reply denies the veracity of the source claim\n",
      "    - QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
      "    - COMMENT: The reply makes their own comment without a clear contribution to assessing the veracity of the source claim\n",
      "    \n",
      "    Respond with ONLY one word: SUPPORT, DENY, QUERY, or COMMENT<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Text: [TARGET] Store owner told @USER Monday that there was a theft , but said it was not MichaelBrown ; said it was someone else #Ferguson\n",
      "\n",
      "Task: Classify the stance of [TARGET] towards the veracity of the rumour in [SRC].<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "SUPPORT<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Text: [SRC] Only photo I will tweet . CPR being performed on the soldier now . I heard four shots . #ottawa HTTPURL [PARENT] @USER so .. Instead of helping you took a picture for profit ? [TARGET] @USER Fuck you idiot\n",
      "\n",
      "Task: Classify the stance of [TARGET] towards the veracity of the rumour in [SRC].<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "DENY<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Text: [SRC] AFP reports there are 2 dead , and 5 hostages being held in the Kosher store in Eastern Paris ; separate incident to charlie hebdo shooters . [PARENT] @USER @USER That 's what the police are saying . I meant ' separate ' as in different guys to the ones that did Charlie Hebdo attack . [TARGET] @USER Thank you , Shiraz . I understand . What do you each think about all this ? What is your reaction ? @USER\n",
      "\n",
      "Task: Classify the stance of [TARGET] towards the veracity of the rumour in [SRC].<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "QUERY<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Text: [SRC] #SYDNEYSIEGE : Hostage-takers ‘ have suicide belts , demand radio convo with #Abbott ’ – report HTTPURL HTTPURL [TARGET] @USER Fuck religion . #sydneySeige\n",
      "\n",
      "Task: Classify the stance of [TARGET] towards the veracity of the rumour in [SRC].<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "COMMENT<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Text: [SRC] Flight #4U9525 initially climbed to 38,000 feet before before it started to descend and lost signal at 6,800 feet . HTTPURL [CTX] @USER But no emergency call made ? Or emergency squawk ? @USER [SEP] @USER @USER @USER Sent Mayday through radio . I suppose he had other things to do than adjust the Squawk [SEP] @USER @USER @USER It turns out there was no distress call made . I 'm almost certain this was a hi-jacking now . [PARENT] @USER @USER @USER Autopilot was manually changed from 38,000 to 100 ft at 09:30 : 55 ( Flightradar 24 says ) [TARGET] @USER the flight profile . That does n't indicate that the autopilot was set to 100 feet . I 've worked on these systems , tested them\n",
      "\n",
      "Task: Classify the stance of [TARGET] towards the veracity of the rumour in [SRC].<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Ground truth: comment\n",
      "Response:     SUPPORT\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c4286e62144ce1911c4cfc29007c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (few-shot):   0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG ===\n",
      "Total raw_responses: 281\n",
      "First 5 raw responses: ['SUPPORT', 'SUPPORT', 'SUPPORT', 'SUPPORT', 'SUPPORT']\n",
      "First 5 predictions: ['support', 'support', 'support', 'support', 'support']\n",
      "First 5 true labels: ['support', 'comment', 'comment', 'comment', 'comment']\n",
      "None count in predictions: 0\n",
      "\n",
      "============================================================\n",
      "Results (few-shot)\n",
      "============================================================\n",
      "Macro F1: 0.2058\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.4039\n",
      "  deny: 0.1429\n",
      "  query: 0.0690\n",
      "  comment: 0.2075\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.26      0.90      0.40        69\n",
      "        deny       0.33      0.09      0.14        11\n",
      "       query       1.00      0.04      0.07        28\n",
      "     comment       0.56      0.13      0.21       173\n",
      "\n",
      "    accuracy                           0.31       281\n",
      "   macro avg       0.54      0.29      0.21       281\n",
      "weighted avg       0.52      0.31      0.24       281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# few-shot\n",
    "\n",
    "row = dev_df.iloc[102]\n",
    "df = dev_df\n",
    "\n",
    "few_shot_examples = get_few_shot_examples(train_df, n_per_class=1)\n",
    "input_text = format_input_with_context(row, df, use_features=False)\n",
    "messages = build_few_shot_messages(input_text, few_shot_examples)\n",
    "\n",
    "\n",
    "# prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# print(prompt)\n",
    "# print()\n",
    "\n",
    "response = generate_response(pipe, messages)\n",
    "label = parse_stance_response(response)\n",
    "print(f\"Ground truth: {row['label_text']}\")\n",
    "# print(f\"Predicted:    {label}\")\n",
    "print(f\"Response:     {response}\")\n",
    "print()\n",
    "\n",
    "few_results = evaluate_prompting(pipe, df, mode=\"few-shot\", examples=few_shot_examples)\n",
    "\n",
    "\n",
    "# print(f\"\\n{'='*60}\\nSummary\\n{'='*60}\")\n",
    "# print(f\"Zero-Shot Macro F1: {zero_results['macro_f1']:.4f}\")\n",
    "# print(f\"Few-Shot Macro F1:  {few_results['macro_f1']:.4f}\")\n",
    "# print(f\"Improvement: {(few_results['macro_f1'] - zero_results['macro_f1'])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@marcepa49 the flight profile. That doesn't indicate that the autopilot was set to 100 feet. I've worked on these systems, tested them\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
