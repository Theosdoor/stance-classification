{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import outlines # see https://dottxt-ai.github.io/outlines/latest/features/models/transformers/\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "\n",
    "from data_loader import load_dataset, format_input_with_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from saved_data/datasets.pkl...\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "USE_CTX = False # whether to use [CTX] parts of threads (too slow for True)\n",
    "MAX_COT_TOKENS = 200 # max n tokens for cot response\n",
    "VALID_STANCES = ['SUPPORT', 'DENY', 'QUERY', 'COMMENT']\n",
    "RELOAD_MODEL = False # takes up lots of mem if we keep reloading\n",
    "\n",
    "RESULTS_DIR = \"./results/prompting/\"\n",
    "RANDOM_SEED = 42\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# load data\n",
    "train_df, dev_df, test_df = load_dataset()\n",
    "use_df = dev_df  # df to test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76158864033b4fa58f40c098c1549979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "# output types for outlines\n",
    "StanceLabel = Literal[\"SUPPORT\", \"DENY\", \"QUERY\", \"COMMENT\"]\n",
    "COT_REGEX = outlines.types.Regex(r\"[\\s\\S]*Label: (SUPPORT|DENY|QUERY|COMMENT)\")\n",
    "\n",
    "if 'model' not in globals() or RELOAD_MODEL:\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # use outlines\n",
    "    model = outlines.from_transformers(hf_model, tokenizer)\n",
    "    \n",
    "    # get rid of annoying warnings re pad_token\n",
    "    model.tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.tokenizer.padding_side = \"left\"\n",
    "    model.model.generation_config.pad_token_id = model.tokenizer.pad_token_id\n",
    "\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "# see https://huggingface.co/docs/transformers/en/tasks/prompting\n",
    "# https://www.promptingguide.ai/techniques/zeroshot\n",
    "\n",
    "PERSONA = \"\"\"You are an expert in rumour stance analysis on twitter.\"\"\"\n",
    "INSTRUCTION = \"\"\"Your task is to classify the stance of the [TARGET] tweet towards the veracity of the rumour in the [SRC] tweet.\"\"\"\n",
    "INPUT_FORMAT = \"\"\"\\\n",
    "The input will be provided as a single string containing labeled segments:\n",
    "\"[SRC] ... [PARENT] ... [TARGET] ...\". (Note: [PARENT] is omitted if [TARGET] replies directly to [SRC])\n",
    "\"\"\"\n",
    "LABEL_DEFNS = \"\"\"\\\n",
    "Classification Labels:\n",
    "- SUPPORT: The reply explicitly supports or provides evidence for the veracity of the source claim\n",
    "- DENY: The tweet explicitly denies or provides counter-evidence for the veracity of the source claim\n",
    "- QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
    "- COMMENT: The reply makes their own opinionated/neutral comment without a clear contribution to assessing the veracity of the source claim\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "\"\"\"\n",
    "# output format set by outlines\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Text: {thread_context}\n",
    "\n",
    "Classify the stance of [TARGET] towards the veracity of the rumour in [SRC]:\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(thread_context):\n",
    "    return USER_PROMPT_TEMPLATE.format(thread_context=thread_context)\n",
    "\n",
    "\n",
    "def build_messages(thread_context, examples=None, sys_prompt=None):\n",
    "    init_prompt = sys_prompt if sys_prompt is not None else SYS_PROMPT # allow custom sys prompt (eg. for CoT/ablations)\n",
    "    messages = [{\"role\": \"system\", \"content\": init_prompt}]\n",
    "    if examples: # for few-shot\n",
    "        for ex in examples:\n",
    "            messages.append({\"role\": \"user\", \"content\": build_user_prompt(ex['source'])})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ex['response']})\n",
    "    messages.append({\"role\": \"user\", \"content\": build_user_prompt(thread_context)}) # final user prompt to answer\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COT prompts \n",
    "\n",
    "# add output format to think step by step\n",
    "COT_OUTPUT_FORMAT = \"\"\"\\\n",
    "Your response should follow this EXACT format:\n",
    "Reasoning: [VERY BRIEFLY explain why the label fits. Use a two stage strategy: Stage 1 classify Stance vs Non-Stance - If Non-stance, skip stage 2 and answer 'COMMENT'. Otherwise: Stage 2 classify Support / Deny / Query]\n",
    "Label: [EXACTLY one of: SUPPORT, DENY, QUERY, or COMMENT]\n",
    "\"\"\"\n",
    "\n",
    "COT_SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "{COT_OUTPUT_FORMAT}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# for few-shot cot\n",
    "\n",
    "# handselected ids for reasoning\n",
    "COT_FEW_SHOT_IDS = {\n",
    "    'support': '524967134339022848', # @TheKirkness Radio Canada tweeting same. must be true :-(\n",
    "    'deny': '544292581950357504', # @JSchoenberger7 @TheAnonMessage not an Isis flag. Just an Islamic one. Stop spreading false rumors.\n",
    "    'query': '544281192632053761', # @mscott Are your reporters 100% sure it is an ISIS flag? Cause that is what is being reported. #facts\n",
    "    'comment': '552804023389392896', # @thei100 @Independent these fuckers thinking its a GTA heist mission\n",
    "}\n",
    "\n",
    "COT_EXAMPLES = {\n",
    "    'support': \"1. Stance: takes position on veracity\\n2. \\\"must be true\\\" → affirms claim\\nLabel: SUPPORT\",\n",
    "    'deny': \"1. Stance: challenges veracity\\n2. \\\"not an Isis flag\\\", \\\"false rumors\\\" → rejects claim\\nLabel: DENY\",\n",
    "    'query': \"1. Stance: engages with veracity\\n2. \\\"Are your reporters 100% sure?\\\" → requests evidence\\nLabel: QUERY\",\n",
    "    'comment': \"1. Stance: no veracity assessment\\n2. Offers opinion/reaction only\\nLabel: COMMENT\",\n",
    "}\n",
    "\n",
    "def get_cot_examples(train_df):\n",
    "    examples = []\n",
    "    for label, tweet_id in COT_FEW_SHOT_IDS.items():\n",
    "        matches = train_df[train_df['tweet_id'] == tweet_id]\n",
    "        if len(matches) == 0:\n",
    "            print(f\"Warning: COT example tweet_id {tweet_id} not found\")\n",
    "            continue\n",
    "        row = matches.iloc[0]\n",
    "        examples.append({\n",
    "            'source': format_input_with_context(row, train_df, use_features=False, use_context=USE_CTX),\n",
    "            'response': COT_EXAMPLES[label]\n",
    "        })\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cot_label(text): # get final label from cot response\n",
    "    match = re.search(r'Label:\\s*(SUPPORT|DENY|QUERY|COMMENT)', text, re.IGNORECASE)\n",
    "    return match.group(1).lower() if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LLM --\n",
    "def plot_confusion_matrix(cm, mode, save_path=None):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix ({mode})')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_results(results, model_key, mode):\n",
    "    \"\"\"Save evaluation results to JSON.\"\"\"\n",
    "    output = {\n",
    "        'model': model_key,\n",
    "        'mode': mode,\n",
    "        'macro_f1': results['macro_f1'],\n",
    "        'per_class_f1': results['per_class_f1'],\n",
    "        'predictions': results['predictions'],\n",
    "        'true_labels': results['true_labels'],\n",
    "        'raw_responses': results['raw_responses']\n",
    "    }\n",
    "    filename = f\"{RESULTS_DIR}{model_key}_{mode}_results.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot examples\n",
    "\n",
    "# from different sources and topics\n",
    "DIVERSE_FEW_SHOT_IDS = {\n",
    "    'support': '553486439129038848', # 'BREAKING: French police tell AP suspects in #CharlieHebdo attack have taken a hostage northeast of Paris.'\n",
    "    'deny': '500384255814299648', # '@TPM @McBlondeLand Awful! The DOJ needs to do thorough investigation of this PD-these problems are systemic. The citizens deserve better!'\n",
    "    'query': '525003827494531073', # '@CTVNews @ctvsaskatoon so what happened at Rideau? Nothing?'\n",
    "    'comment': '544314008980172800', # \"@WilliamsJon @morgfair how do we know the guy isn't just pissed that he got fired or foreclosed on. Or his gf is cheating on him.\"\n",
    "}\n",
    "    \n",
    "# From same source thread\n",
    "SAME_SRC_SOURCE_ID = '529739968470867968' # 'Prince not playing Massey Hall tonight, promoter says http://t.co/clVHvM07kx http://t.co/IADirsFOOC'\n",
    "SAME_SRC_FEW_SHOT_IDS = {\n",
    "    'support': '529740822238232577', # 'athenaph what tease!!! RT @CBCNews: Prince not playing Massey Hall tonight, promoter says http://t.co/245JKmwWHH'\n",
    "    'deny': '529740809672077313', # '.@CBCNews I can also confirm he will not be playing in Rankin Inlet, NU either'\n",
    "    'query': '529748991849013248', # '@CBCNews And, ... ?\\nWhen @CBCNews asked Live Nation why no one made this clarification 4 or 5 hours earlier?'\n",
    "    'comment': '529741574742507520', # '@CBCNews  lol.'\n",
    "}\n",
    "\n",
    "# random stratified selection (one per stance but randomly picked)\n",
    "RANDOM_STRATIFIED_FEW_SHOT_IDS = {\n",
    "    'support': '544306719686656000', # 'US evacuated Consulate in Sydney after #sydneysiege. Emergency warning to U.S. citizens urging them to \"maintain high level of vigilance\".'\n",
    "    'deny': '524990163446140928', # 'Police have clarified that there were two shootings in Ottawa today, not three: at the War Memorial and Parliament Hill.'\n",
    "    'query': '500386447158161408', # '@TPM Wow. I thought a Tweep was being sarcastic, but...it really happened that way?!?'\n",
    "    'comment': '553543395717550080', # '@FoxNews @Machma7Machos the terrorists need empathy send them to Harvard University'\n",
    "}\n",
    "\n",
    "FEW_SHOT_SETS = {\n",
    "    'diverse': DIVERSE_FEW_SHOT_IDS,\n",
    "    'same_src': SAME_SRC_FEW_SHOT_IDS,\n",
    "    'random': RANDOM_STRATIFIED_FEW_SHOT_IDS,\n",
    "}\n",
    "\n",
    "\n",
    "def get_few_shot_examples(df, n_per_class=1, use_set=None):\n",
    "    examples = []\n",
    "    \n",
    "    if use_set is not None:\n",
    "        # deterministic using defined sets\n",
    "        if use_set not in FEW_SHOT_SETS:\n",
    "            raise ValueError(f\"Unknown use_set '{use_set}'. Choose from: {list(FEW_SHOT_SETS.keys())}\")\n",
    "        \n",
    "        id_dict = FEW_SHOT_SETS[use_set]\n",
    "        for label, tweet_id in id_dict.items():\n",
    "            matches = df[df['tweet_id'] == tweet_id]\n",
    "            if len(matches) == 0:\n",
    "                print(f\"Warning: Few-shot example tweet_id {tweet_id} not found in df\")\n",
    "                continue\n",
    "            row = matches.iloc[0]\n",
    "            examples.append({\n",
    "                'source': format_input_with_context(row, df, use_features=False, use_context=USE_CTX),\n",
    "                'response': label.upper()\n",
    "            })\n",
    "    else:\n",
    "        # Non-deterministic: random stratified sampling\n",
    "        for label in ['support', 'deny', 'query', 'comment']:\n",
    "            class_df = df[df['label_text'] == label]\n",
    "            samples = class_df.sample(n=min(n_per_class, len(class_df)))\n",
    "            \n",
    "            for _, row in samples.iterrows():\n",
    "                examples.append({\n",
    "                    'source': format_input_with_context(row, df, use_features=False, use_context=USE_CTX),\n",
    "                    'response': label.upper()\n",
    "                })\n",
    "    \n",
    "    return examples\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompting(model, df, mode=\"zero-shot\", examples=None, sys_prompt_override=None, verbose=True):\n",
    "    # set output params\n",
    "    if mode == \"cot\":\n",
    "        output_type = COT_REGEX\n",
    "        max_new_tokens = MAX_COT_TOKENS\n",
    "    else:\n",
    "        output_type = StanceLabel\n",
    "        max_new_tokens = 10\n",
    "        \n",
    "    # check mode / get correct sys prompt for messages\n",
    "    if sys_prompt_override is not None: # for ablations\n",
    "        active_sys_prompt = sys_prompt_override\n",
    "    elif mode == \"cot\":\n",
    "        active_sys_prompt = COT_SYS_PROMPT\n",
    "    elif mode in (\"zero-shot\", \"few-shot\"):\n",
    "        active_sys_prompt = SYS_PROMPT # standard\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognised mode: {mode}. Choose from 'zero-shot', 'few-shot', or 'cot'.\")\n",
    "    \n",
    "    # get all prompts before\n",
    "    all_prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        input_text = format_input_with_context(row, df, use_features=False, use_context=USE_CTX)\n",
    "        messages = build_messages(input_text, examples=examples, sys_prompt=active_sys_prompt)\n",
    "        \n",
    "#         if mode == 'cot':\n",
    "#             messages[-1]['content'] += \"\\nLet's think step-by-step.\" # for cot\n",
    "        \n",
    "        # get prompt string from messages\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        all_prompts.append(prompt)\n",
    "    \n",
    "    # get responses\n",
    "    raw_responses = []\n",
    "    predictions = []\n",
    "    \n",
    "    for prompt in tqdm(all_prompts, desc=f\"Evaluating ({mode})\"):\n",
    "        response = model(prompt, output_type, max_new_tokens=max_new_tokens)\n",
    "        raw_responses.append(response)\n",
    "        \n",
    "        if mode == \"cot\":\n",
    "            pred = parse_cot_label(response) # may return None\n",
    "        else:\n",
    "            pred = response.lower() # outlines fixes to a valid stance\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # parsing errors (only for cot)\n",
    "    if mode == 'cot':\n",
    "        error_idxs = [i for i, p in enumerate(predictions) if p is None]\n",
    "        if error_idxs and verbose:\n",
    "            print(f\"Warning: {len(error_idxs)} errors in predictions. Replacing with 'comment'.\")\n",
    "            for idx in error_idxs[:5]:  # Show first 5 errors only\n",
    "                print(f\"Index: {idx}\\nRaw Model Output: '{raw_responses[idx]}'\\n\" + \"-\" * 30)\n",
    "\n",
    "        # set errors to comment (majority class)\n",
    "        predictions = [p if p is not None else 'comment' for p in predictions]\n",
    "        \n",
    "    true_labels = df['label_text'].tolist()\n",
    "    \n",
    "    # metrics\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    per_class_f1 = f1_score(true_labels, predictions, average=None, labels=labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\\nResults ({mode})\\n{'='*60}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\\n\\nPer-class F1:\")\n",
    "        for lbl, f1 in zip(labels, per_class_f1):\n",
    "            print(f\"  {lbl}: {f1:.4f}\")\n",
    "        print(f\"\\n{classification_report(true_labels, predictions, labels=labels, zero_division=0.0)}\")\n",
    "    \n",
    "    # save and return predictions\n",
    "    results_df = pd.DataFrame({'true_label': true_labels, 'predicted': predictions})\n",
    "    csv_path = f\"{RESULTS_DIR}predictions_{mode}.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    if verbose:\n",
    "        print(f\"Saved predictions: {csv_path}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'raw_responses': raw_responses,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': dict(zip(labels, per_class_f1)),\n",
    "        'confusion_matrix': confusion_matrix(true_labels, predictions, labels=labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0df239629e4ea4af843dec49837d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (zero-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results (zero-shot)\n",
      "============================================================\n",
      "Macro F1: 0.1008\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.0000\n",
      "  deny: 0.1714\n",
      "  query: 0.2204\n",
      "  comment: 0.0115\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.00      0.00      0.00        69\n",
      "        deny       0.10      0.55      0.17        11\n",
      "       query       0.12      0.96      0.22        28\n",
      "     comment       1.00      0.01      0.01       173\n",
      "\n",
      "    accuracy                           0.12       281\n",
      "   macro avg       0.31      0.38      0.10       281\n",
      "weighted avg       0.63      0.12      0.04       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_zero-shot.csv\n"
     ]
    }
   ],
   "source": [
    "# zero-shot\n",
    "zero_results = evaluate_prompting(model, use_df, mode=\"zero-shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab03127fce34cefaf08ce70fe57b58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Isolated Ablation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c853b0b45c1d4c8586c8d75ee887b54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (zero-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fig\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Run isolated ablation (each component individually)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m isolated_results = \u001b[43mrun_ablation_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mISOLATED_CONFIGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIsolated Ablation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m plot_ablation(isolated_results, \u001b[33m'\u001b[39m\u001b[33mSystem Prompt Ablation: Isolated Component Contribution\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    122\u001b[39m               save_path=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRESULTS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mablation_isolated.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Run cumulative ablation (stacking components)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mrun_ablation_study\u001b[39m\u001b[34m(model, df, configs, desc)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m config_name, component_keys \u001b[38;5;129;01min\u001b[39;00m tqdm(configs.items(), desc=desc):\n\u001b[32m     54\u001b[39m     sys_prompt = build_ablation_sys_prompt(component_keys)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     ablation_results = \u001b[43mevaluate_prompting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzero-shot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43msys_prompt_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     results[config_name] = ablation_results[\u001b[33m'\u001b[39m\u001b[33mmacro_f1\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mablation_results[\u001b[33m'\u001b[39m\u001b[33mmacro_f1\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mevaluate_prompting\u001b[39m\u001b[34m(model, df, mode, examples, sys_prompt_override, verbose)\u001b[39m\n\u001b[32m     32\u001b[39m predictions = []\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m tqdm(all_prompts, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     response = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     raw_responses.append(response)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mcot\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/outlines/models/base.py:122\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, model_input, output_type, backend, **inference_kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call the model.\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m     89\u001b[39m \u001b[33;03mUsers can call the model directly, in which case we will create a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m \n\u001b[32m    119\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moutlines\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Generator\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m(model_input, **inference_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/outlines/generator.py:392\u001b[39m, in \u001b[36mGenerator\u001b[39m\u001b[34m(model, output_type, backend, processor)\u001b[39m\n\u001b[32m    390\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m SteerableGenerator.from_processor(model, processor) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSteerableGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/outlines/generator.py:253\u001b[39m, in \u001b[36mSteerableGenerator.__init__\u001b[39m\u001b[34m(self, model, output_type, backend_name)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    252\u001b[39m     regex_string = to_regex(term)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28mself\u001b[39m.logits_processor = \u001b[43mget_regex_logits_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregex_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/outlines/backends/__init__.py:109\u001b[39m, in \u001b[36mget_regex_logits_processor\u001b[39m\u001b[34m(backend_name, model, regex)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_regex_logits_processor\u001b[39m(\n\u001b[32m     88\u001b[39m     backend_name: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     89\u001b[39m     model: SteerableModel,\n\u001b[32m     90\u001b[39m     regex: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     91\u001b[39m ) -> LogitsProcessorType:\n\u001b[32m     92\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a logits processor from a regex.\u001b[39;00m\n\u001b[32m     93\u001b[39m \n\u001b[32m     94\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     backend = \u001b[43m_get_backend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mREGEX_DEFAULT_BACKEND\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m backend.get_regex_logits_processor(regex)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/outlines/backends/__init__.py:49\u001b[39m, in \u001b[36m_get_backend\u001b[39m\u001b[34m(backend_name, model)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a Backend instance.\u001b[39;00m\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m \n\u001b[32m     47\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend_name == \u001b[33m\"\u001b[39m\u001b[33moutlines_core\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOutlinesCoreBackend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m backend_name == \u001b[33m\"\u001b[39m\u001b[33mxgrammar\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m XGrammarBackend(model)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Component dictionary for easy reference (no OUTPUT_FORMAT - only used for CoT)\n",
    "PROMPT_COMPONENTS = {\n",
    "    'persona': PERSONA,\n",
    "    'instruction': INSTRUCTION,\n",
    "    'input_format': INPUT_FORMAT,\n",
    "    'label_defns': LABEL_DEFNS,\n",
    "}\n",
    "\n",
    "# Build colour palette: light grey for minimal, seaborn default for components, black for full\n",
    "_default_palette = sns.color_palette()\n",
    "COMPONENT_COLOURS = {\n",
    "    'minimal': '#D3D3D3',\n",
    "    'persona': _default_palette[0], #1f77b4\n",
    "    'instruction': _default_palette[1], #ff7f0e\n",
    "    'input_format': _default_palette[2], #2ca02c\n",
    "    'label_defns': _default_palette[3], #d62728\n",
    "    'full': '#000000',\n",
    "}\n",
    "\n",
    "# ISOLATED: minimal + ONE component (shows individual contribution)\n",
    "ISOLATED_CONFIGS = {\n",
    "    'minimal': [],\n",
    "    '+persona': ['persona'],\n",
    "    '+instruction': ['instruction'],\n",
    "    '+input_format': ['input_format'],\n",
    "    '+label_defns': ['label_defns'],\n",
    "    'full': ['persona', 'instruction', 'input_format', 'label_defns'],\n",
    "}\n",
    "\n",
    "# CUMULATIVE: stacking components one by one\n",
    "CUMULATIVE_CONFIGS = {\n",
    "    'minimal': [],\n",
    "    '+persona': ['persona'],\n",
    "    '+instruction': ['persona', 'instruction'],\n",
    "    '+input_format': ['persona', 'instruction', 'input_format'],\n",
    "    '+label_defns (full)': ['persona', 'instruction', 'input_format', 'label_defns'],\n",
    "}\n",
    "\n",
    "\n",
    "def build_ablation_sys_prompt(component_keys):\n",
    "    \"\"\"Build system prompt from list of component keys.\"\"\"\n",
    "    if not component_keys:\n",
    "        return \"\"  # Empty system prompt for minimal\n",
    "    return \"\\n\".join(PROMPT_COMPONENTS[k] for k in component_keys)\n",
    "\n",
    "\n",
    "def run_ablation_study(model, df, configs, desc=\"Ablation\"):\n",
    "    \"\"\"Run ablation study with given configurations. Returns dict of {config_name: macro_f1}.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for config_name, component_keys in tqdm(configs.items(), desc=desc):\n",
    "        sys_prompt = build_ablation_sys_prompt(component_keys)\n",
    "        ablation_results = evaluate_prompting(\n",
    "            model, df, mode=\"zero-shot\", examples=None, \n",
    "            sys_prompt_override=sys_prompt, verbose=False\n",
    "        )\n",
    "        results[config_name] = ablation_results['macro_f1']\n",
    "        print(f\"  {config_name}: {ablation_results['macro_f1']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _get_ablation_color(config_name):\n",
    "    \"\"\"Get bar color based on config name.\"\"\"\n",
    "    if config_name == 'minimal':\n",
    "        return COMPONENT_COLOURS['minimal']\n",
    "    elif 'full' in config_name:\n",
    "        return COMPONENT_COLOURS['full']\n",
    "    for comp in COMPONENT_COLOURS:\n",
    "        if comp in config_name:\n",
    "            return COMPONENT_COLOURS[comp]\n",
    "    return COMPONENT_COLOURS['minimal']\n",
    "\n",
    "\n",
    "def plot_ablation(results, title, save_path=None, show_delta=False):\n",
    "    \"\"\"Plot horizontal bar chart for ablation results.\"\"\"\n",
    "    df_plot = pd.DataFrame({\n",
    "        'config': list(results.keys()),\n",
    "        'score': list(results.values())\n",
    "    })\n",
    "    df_plot['color'] = [_get_ablation_color(c) for c in df_plot['config']]\n",
    "    palette = dict(zip(df_plot['config'], df_plot['color']))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.barplot(data=df_plot, y='config', x='score', hue='config', palette=palette, \n",
    "                ax=ax, edgecolor='white', legend=False)\n",
    "    \n",
    "    ax.set_xlabel('Macro F1 Score')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_title(f'{title}\\n(Zero-Shot on Dev Set)')\n",
    "    ax.set_xlim(0, df_plot['score'].max() * 1.15)\n",
    "    \n",
    "    for i, (score, config) in enumerate(zip(df_plot['score'], df_plot['config'])):\n",
    "        ax.text(score + 0.005, i, f'{score:.3f}', va='center', fontsize=10)\n",
    "    \n",
    "    if 'minimal' in results:\n",
    "        ax.axvline(x=results['minimal'], color=COMPONENT_COLOURS['minimal'], \n",
    "                   linestyle='--', alpha=0.7, linewidth=1.5, label='Minimal baseline')\n",
    "        ax.legend(loc='lower right')\n",
    "    \n",
    "    if show_delta:\n",
    "        scores = list(results.values())\n",
    "        for i in range(1, len(scores)):\n",
    "            delta = scores[i] - scores[i-1]\n",
    "            sign = '+' if delta >= 0 else ''\n",
    "            ax.text(scores[i] + 0.04, i - 0.25, f'{sign}{delta:.3f}', \n",
    "                    fontsize=9, color='dimgray', style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Run isolated ablation (each component individually)\n",
    "isolated_results = run_ablation_study(model, use_df, ISOLATED_CONFIGS, desc=\"Isolated Ablation\")\n",
    "plot_ablation(isolated_results, 'System Prompt Ablation: Isolated Component Contribution', \n",
    "              save_path=f\"{RESULTS_DIR}ablation_isolated.png\")\n",
    "\n",
    "# Run cumulative ablation (stacking components)\n",
    "cumulative_results = run_ablation_study(model, use_df, CUMULATIVE_CONFIGS, desc=\"Cumulative Ablation\")\n",
    "plot_ablation(cumulative_results, 'System Prompt Ablation: Cumulative Component Stacking', \n",
    "              save_path=f\"{RESULTS_DIR}ablation_cumulative.png\", show_delta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cde76a0d87949a399aaaf8a022e014e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (few-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results (few-shot)\n",
      "============================================================\n",
      "Macro F1: 0.3789\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.3977\n",
      "  deny: 0.2963\n",
      "  query: 0.4065\n",
      "  comment: 0.4153\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.33      0.51      0.40        69\n",
      "        deny       0.25      0.36      0.30        11\n",
      "       query       0.26      0.89      0.41        28\n",
      "     comment       0.78      0.28      0.42       173\n",
      "\n",
      "    accuracy                           0.40       281\n",
      "   macro avg       0.40      0.51      0.38       281\n",
      "weighted avg       0.60      0.40      0.41       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_few-shot.csv\n"
     ]
    }
   ],
   "source": [
    "# few-shot\n",
    "few_shot_examples = get_few_shot_examples(train_df, n_per_class=1)\n",
    "few_results = evaluate_prompting(model, use_df, mode=\"few-shot\", examples=few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f222a9d6c1514600b7847a826937ffda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (cot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 3 errors in predictions. Replacing with 'comment'.\n",
      "Index: 65\n",
      "Raw Model Output: 'Reasoning: Stage 1: Stance vs Non-Stance - [TARGET] is responding to the source claim, so it is a stance. Stage 2: Support / Deny / Query - The reply does not explicitly support or deny the veracity of the source claim, nor does it ask for additional evidence. Instead, it makes a comment, \"another great tragedy\", which is an opinionated statement. Therefore, the label is: COMMENT. [COMMENT] The reply is a neutral comment without a clear contribution to assessing the veracity of the source claim. It does not provide any direct support, denial, or query, but rather expresses sympathy or shock. The tone is also somewhat informal, which is typical of a condolence message. The reply does not directly engage with the veracity of the claim, but rather reacts to the news. Therefore, the reply does not fit into the categories of Support, Deny, or Query. A more fitting response might have been \"Did'\n",
      "------------------------------\n",
      "Index: 95\n",
      "Raw Model Output: 'Reasoning: Stage 1: Non-Stance - [TARGET] does not directly respond to the veracity of the claim in [SRC], but rather provides additional information that appears to contradict the number of passengers on board. This suggests a neutral or informative tone rather than a stance on the veracity of the claim. Stage 2: CLASSIFY: COMMENT - [TARGET] provides new information that does not directly support, deny, or query the original claim, but rather offers an alternative piece of information that could be seen as a fact-check or correction. However, given the context of the original tweet being a breaking news report, it's more likely that [TARGET] is simply providing an update or correction rather than a neutral comment. A more accurate classification would be CLASSIFY: COMMENT. But given the context, I will classify it as SUPPORT as [TARGET] is providing factual information that updates the original claim, which can be seen as supporting the veracity of the original claim. However,'\n",
      "------------------------------\n",
      "Index: 111\n",
      "Raw Model Output: 'Reasoning: Stage 1: Stance vs Non-Stance - Since [TARGET] responds directly to [SRC] and does not make a general comment, it is a stance. Stage 2: Support / Deny / Query - [TARGET] does not explicitly support or deny the claim, but rather provides an alternative explanation. It neither confirms nor denies the original claim, so it can be classified as neither SUPPORT nor DENY. The tone is more inquiring, so it is neither SUPPORT nor DENY. However, it does provide a counter-narrative to the original claim, so it is neither SUPPORT nor DENY. The best fit is QUERY, but since it provides a narrative, it is more of a COMMENT. Therefore, the label is COMMENT. However, a better fit is actually DENY, as it does provide a narrative that directly counters the original claim. So, the correct answer is DENY. The comment actually counters the original claim by providing an alternative explanation.'\n",
      "------------------------------\n",
      "\n",
      "============================================================\n",
      "Results (cot)\n",
      "============================================================\n",
      "Macro F1: 0.3777\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.3359\n",
      "  deny: 0.1176\n",
      "  query: 0.4308\n",
      "  comment: 0.6265\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.35      0.32      0.34        69\n",
      "        deny       0.09      0.18      0.12        11\n",
      "       query       0.38      0.50      0.43        28\n",
      "     comment       0.65      0.60      0.63       173\n",
      "\n",
      "    accuracy                           0.51       281\n",
      "   macro avg       0.37      0.40      0.38       281\n",
      "weighted avg       0.53      0.51      0.52       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_cot.csv\n"
     ]
    }
   ],
   "source": [
    "# CoT prompting\n",
    "\n",
    "cot_results = evaluate_prompting(model, use_df, mode=\"cot\", examples=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Jan 2026\n",
      "\n",
      "You are an expert in rumour stance analysis on twitter.\n",
      "Your task is to classify the stance of the [TARGET] tweet towards the veracity of the rumour in the [SRC] tweet.\n",
      "The input will be provided as a single string containing labeled segments:\n",
      "\"[SRC] ... [PARENT] ... [TARGET] ...\". (Note: [PARENT] is omitted if [TARGET] replies directly to [SRC])\n",
      "\n",
      "Classification Labels:\n",
      "- SUPPORT: The reply explicitly supports or provides evidence for the veracity of the source claim\n",
      "- DENY: The tweet explicitly denies or provides counter-evidence for the veracity of the source claim\n",
      "- QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
      "- COMMENT: The reply makes their own opinionated/neutral comment without a clear contribution to assessing the veracity of the source claim\n",
      "\n",
      "Your response should follow this EXACT format:\n",
      "Reasoning: [VERY BRIEFLY explain why the label fits. Use a two stage strategy: Stage 1 classify Stance vs Non-Stance - If Non-stance, skip stage 2 and answer 'COMMENT'. Otherwise: Stage 2 classify Support / Deny / Query]\n",
      "Label: [EXACTLY one of: SUPPORT, DENY, QUERY, or COMMENT]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Text: [SRC] Heart goes out to 148 passengers and crew of Germanwings Airbus A320 that has crashed in French Alps , Southern France HTTPURL [TARGET] @USER So sad to know that it was a deliberate suicide\n",
      "\n",
      "Classify the stance of [TARGET] towards the veracity of the rumour in [SRC]:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Response: Reasoning: Stage 1: Stance vs Non-Stance - The reply directly responds to the source claim, so it's a stance. Stage 2: Support / Deny / Query - The reply explicitly states \"deliberate suicide\", which is a direct counter-statement to the reported incident being an accident, thereby denying the veracity of the source claim. Label: DENY\n",
      "\n",
      "Prediction: deny\n",
      "True label: comment\n"
     ]
    }
   ],
   "source": [
    "# test a single eg\n",
    "row = dev_df.iloc[5]\n",
    "input_text = format_input_with_context(row, dev_df, use_features=False, use_context=USE_CTX)\n",
    "messages = build_messages(input_text, None, COT_SYS_PROMPT) \n",
    "# messages[-1]['content'] += \"\\nLet's think step-by-step.\" # for cot\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "response = model(prompt, COT_REGEX, max_new_tokens=MAX_COT_TOKENS)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "print(f\"Prediction: {parse_cot_label(response)}\")\n",
    "print(f\"True label: {row['label_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93714374308845c2a2ddd33b52765f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (cot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 140 errors in predictions. Replacing with 'comment'.\n",
      "Index: 4\n",
      "Raw Model Output: '1. Stance: acknowledges veracity indirectly (through shared source URL, but no direct statement on factuality) and expresses sympathy, but doesn't provide any additional information or comment on the incident's veracity itself. However, given the absence of any direct challenge, lack of denial, or query, the sentiment is sympathetic rather than informative or argumentative. It can be argued that it is a neutral comment rather than a stance on the veracity of the claim. However, in the context of your original request, a more fitting response would be to classify it as a COMMENT. Since the target does not directly address the veracity of the claim, the response does not challenge or support the claim but rather provides a sympathetic response. Therefore, a more fitting response would be to label it as COMMENT. However, a more nuanced approach might be to label it as QUERY in the sense that it seeks to understand the context of the incident, but in the context of your original request, it is more'\n",
      "------------------------------\n",
      "Index: 6\n",
      "Raw Model Output: '1. Stance: takes position on veracity (in relation to a different claim, not the main one in [SRC]) but is off-topic to [SRC] and can be disregarded as a non-sequitur for [SRC] classification. However, given the context of the [SRC] it is clear [TARGET] is making a comment about a previous [SRC] which is about plane crash. Hence [TARGET] is making a comment on a related topic. Hence it is safe to disregard [TARGET] for [SRC] classification. However in a different context it would be classified as: 2. Stance: takes position on veracity (in relation to a different claim, not the main one in [SRC]) but is off-topic to [SRC]. The comment is about a different event (Leebowitz) and can be disregarded as non-relevant to [SRC]. However in a different context it would be classified as: 3. Stance: takes'\n",
      "------------------------------\n",
      "Index: 7\n",
      "Raw Model Output: '1. Stance: assesses veracity in relation to additional information provided by [SRC] and adds new information (possibility of survivors) → takes position on veracity of original claim and provides new evidence/assumption to support or contradict it. However, the tone is more speculative than a clear assertion or denial, and doesn't explicitly support or deny the claim, but rather offers a possibility that might be relevant to the veracity of the original claim. However, the tone is more speculative and the language used is not strongly supporting or denying, rather adding a possibility. Therefore, it is slightly leaning towards QUERY but the possibility of survival is added in a somewhat speculative manner and doesn't directly address the veracity of the original claim. But the most fitting label given the context is QUERY, as the statement adds new information and doesn't strongly support or deny the original claim, but it does not explicitly ask for evidence. The statement is more of a speculative comment than a clear stance. However'\n",
      "------------------------------\n",
      "Index: 8\n",
      "Raw Model Output: '1. Stance: takes position on veracity (expressing doubt on terrorism aspect, but not directly addressing the general veracity of the claim about the crash itself) → takes position on veracity in part, but not entirely on the crash itself, so doesn't fully classify as support or deny, but more as a neutral or cautious stance on the terrorism claim specifically, however since [TARGET] doesn't directly address the crash itself but rather the aspect of terrorism, it is more aligned with a cautious stance on a specific aspect of the claim rather than the crash itself, and it is cautious, so it is better classified as a query on the terrorism claim rather than a direct comment on the crash itself, but since it doesn't ask for evidence or clarification on the crash itself, it is more aligned with a cautious stance rather than a query, and since it does express a view on the claim, it is not a comment, it is more aligned with a cautious stance, but since it is'\n",
      "------------------------------\n",
      "Index: 9\n",
      "Raw Model Output: '1. Stance: draws conclusion on veracity based on source claim, and adds information that could be seen as supporting the claim, but the added information is a generalization and not a direct link to the claim. However the claim is not explicitly supported by the added information. The added information is a generalization of the source claim, and the tone of the message is critical of the source claim.  However, the tone of the message is more critical than supportive. This makes the message slightly more than just neutral, but less than supportive. However, the added information can be seen as a generalization of the source claim. The claim is not explicitly supported by the added information. The tone of the message is critical of the source claim, and the added information is a generalization of the source claim. The tone of the message is critical of the source claim, and the added information can be seen as a generalization of the source claim. The tone of the message is critical of the'\n",
      "------------------------------\n",
      "\n",
      "============================================================\n",
      "Results (cot)\n",
      "============================================================\n",
      "Macro F1: 0.4041\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.1875\n",
      "  deny: 0.2727\n",
      "  query: 0.4364\n",
      "  comment: 0.7198\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.33      0.13      0.19        69\n",
      "        deny       0.27      0.27      0.27        11\n",
      "       query       0.44      0.43      0.44        28\n",
      "     comment       0.65      0.81      0.72       173\n",
      "\n",
      "    accuracy                           0.58       281\n",
      "   macro avg       0.42      0.41      0.40       281\n",
      "weighted avg       0.54      0.58      0.54       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_cot.csv\n"
     ]
    }
   ],
   "source": [
    "# few shot cot\n",
    "cot_few_shot_examples = get_cot_examples(train_df)\n",
    "cot_few_results = evaluate_prompting(model, use_df, mode=\"cot\", examples=cot_few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312k",
   "language": "python",
   "name": "venv312k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
