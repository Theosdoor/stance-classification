{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import outlines # see https://dottxt-ai.github.io/outlines/latest/features/models/transformers/\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "\n",
    "from data_loader import load_dataset, format_input_with_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from saved_data/datasets.pkl...\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "USE_CTX = False # whether to use [CTX] parts of threads (too slow for True)\n",
    "MAX_COT_TOKENS = 200 # max n tokens for cot response\n",
    "VALID_STANCES = ['SUPPORT', 'DENY', 'QUERY', 'COMMENT']\n",
    "RELOAD_MODEL = False # takes up lots of mem if we keep reloading\n",
    "\n",
    "RESULTS_DIR = \"./results/prompting/\"\n",
    "RANDOM_SEED = 42\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# load data\n",
    "train_df, dev_df, test_df = load_dataset()\n",
    "use_df = dev_df  # df to test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66b0374bb0042d29332b0f54491f982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "# output types for outlines\n",
    "StanceLabel = Literal[\"SUPPORT\", \"DENY\", \"QUERY\", \"COMMENT\"]\n",
    "COT_REGEX = outlines.types.Regex(r\".*Label: (SUPPORT|DENY|QUERY|COMMENT)\")\n",
    "\n",
    "if 'model' not in globals() or RELOAD_MODEL:\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # use outlines\n",
    "    model = outlines.from_transformers(hf_model, tokenizer)\n",
    "    \n",
    "    # get rid of annoying warnings re pad_token\n",
    "    model.tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.tokenizer.padding_side = \"left\"\n",
    "    model.model.generation_config.pad_token_id = model.tokenizer.pad_token_id\n",
    "\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "# see https://huggingface.co/docs/transformers/en/tasks/prompting\n",
    "# https://www.promptingguide.ai/techniques/zeroshot\n",
    "\n",
    "PERSONA = \"\"\"You are an expert in rumour stance analysis on twitter.\"\"\"\n",
    "INSTRUCTION = \"\"\"Your task is to classify the stance of the [TARGET] tweet towards the veracity of the rumour in the [SRC] tweet.\"\"\"\n",
    "INPUT_FORMAT = \"\"\"\\\n",
    "The input will be provided as a single string containing labeled segments:\n",
    "\"[SRC] ... [PARENT] ... [TARGET] ...\". (Note: [PARENT] is omitted if [TARGET] replies directly to [SRC])\n",
    "\"\"\"\n",
    "LABEL_DEFNS = \"\"\"\\\n",
    "Classification Labels:\n",
    "- SUPPORT: The reply explicitly supports or provides evidence for the veracity of the source claim\n",
    "- DENY: The tweet explicitly denies or provides counter-evidence for the veracity of the source claim\n",
    "- QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
    "- COMMENT: The reply makes their own opinionated/neutral comment without a clear contribution to assessing the veracity of the source claim\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "\"\"\"\n",
    "# output format set by outlines\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Text: {thread_context}\n",
    "\n",
    "Classify the stance of [TARGET] towards the veracity of the rumour in [SRC]:\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(thread_context):\n",
    "    return USER_PROMPT_TEMPLATE.format(thread_context=thread_context)\n",
    "\n",
    "\n",
    "def build_messages(thread_context, examples=None, sys_prompt=None):\n",
    "    init_prompt = sys_prompt if sys_prompt is not None else SYS_PROMPT # allow custom sys prompt (eg. for CoT/ablations)\n",
    "    messages = [{\"role\": \"system\", \"content\": init_prompt}]\n",
    "    if examples: # for few-shot\n",
    "        for ex in examples:\n",
    "            messages.append({\"role\": \"user\", \"content\": build_user_prompt(ex['source'])})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ex['response']})\n",
    "    messages.append({\"role\": \"user\", \"content\": build_user_prompt(thread_context)}) # final user prompt to answer\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COT prompts \n",
    "\n",
    "# add output format to think step by step\n",
    "COT_OUTPUT_FORMAT = \"\"\"\\\n",
    "Your response should follow this EXACT format:\n",
    "Reasoning: [VERY BRIEFLY explain why the label fits. Use a two stage strategy: Stage 1 classify Stance vs Non-Stance - If Non-stance, skip stage 2 and answer 'COMMENT'. Otherwise: Stage 2 classify Support / Deny / Query]\n",
    "Label: [EXACTLY one of: SUPPORT, DENY, QUERY, or COMMENT]\n",
    "\"\"\"\n",
    "\n",
    "COT_SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "{COT_OUTPUT_FORMAT}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# for few-shot cot\n",
    "\n",
    "# handselected ids for reasoning\n",
    "COT_FEW_SHOT_IDS = {\n",
    "    'support': '524967134339022848', # @TheKirkness Radio Canada tweeting same. must be true :-(\n",
    "    'deny': '544292581950357504', # @JSchoenberger7 @TheAnonMessage not an Isis flag. Just an Islamic one. Stop spreading false rumors.\n",
    "    'query': '544281192632053761', # @mscott Are your reporters 100% sure it is an ISIS flag? Cause that is what is being reported. #facts\n",
    "    'comment': '552804023389392896', # @thei100 @Independent these fuckers thinking its a GTA heist mission\n",
    "}\n",
    "\n",
    "COT_EXAMPLES = {\n",
    "    'support': \"1. Stance: takes position on veracity\\n2. \\\"must be true\\\" → affirms claim\\nLabel: SUPPORT\",\n",
    "    'deny': \"1. Stance: challenges veracity\\n2. \\\"not an Isis flag\\\", \\\"false rumors\\\" → rejects claim\\nLabel: DENY\",\n",
    "    'query': \"1. Stance: engages with veracity\\n2. \\\"Are your reporters 100% sure?\\\" → requests evidence\\nLabel: QUERY\",\n",
    "    'comment': \"1. Stance: no veracity assessment\\n2. Offers opinion/reaction only\\nLabel: COMMENT\",\n",
    "}\n",
    "\n",
    "def get_cot_examples(train_df):\n",
    "    examples = []\n",
    "    for label, tweet_id in COT_FEW_SHOT_IDS.items():\n",
    "        matches = train_df[train_df['tweet_id'] == tweet_id]\n",
    "        if len(matches) == 0:\n",
    "            print(f\"Warning: COT example tweet_id {tweet_id} not found\")\n",
    "            continue\n",
    "        row = matches.iloc[0]\n",
    "        examples.append({\n",
    "            'source': format_input_with_context(row, train_df, use_features=False, use_context=USE_CTX),\n",
    "            'response': COT_EXAMPLES[label]\n",
    "        })\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cot_label(text): # get final label from cot response\n",
    "    match = re.search(r'Label:\\s*(SUPPORT|DENY|QUERY|COMMENT)', text, re.IGNORECASE)\n",
    "    return match.group(1).lower() if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LLM --\n",
    "def plot_confusion_matrix(cm, mode, save_path=None):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix ({mode})')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_results(results, model_key, mode):\n",
    "    \"\"\"Save evaluation results to JSON.\"\"\"\n",
    "    output = {\n",
    "        'model': model_key,\n",
    "        'mode': mode,\n",
    "        'macro_f1': results['macro_f1'],\n",
    "        'per_class_f1': results['per_class_f1'],\n",
    "        'predictions': results['predictions'],\n",
    "        'true_labels': results['true_labels'],\n",
    "        'raw_responses': results['raw_responses']\n",
    "    }\n",
    "    filename = f\"{RESULTS_DIR}{model_key}_{mode}_results.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot examples\n",
    "support_example = train_df[train_df['label_text']=='support'].iloc[0]\n",
    "deny_example = train_df[train_df['label_text']=='deny'].iloc[0]\n",
    "query_example = train_df[train_df['label_text']=='query'].iloc[0]\n",
    "comment_example = train_df[train_df['label_text']=='comment'].iloc[0]\n",
    "\n",
    "\n",
    "def get_few_shot_examples(df, n_per_class=1):\n",
    "    \"\"\"Select stratified random examples for few-shot prompting.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for label in ['support', 'deny', 'query', 'comment']:\n",
    "        class_df = df[df['label_text'] == label]\n",
    "        samples = class_df.sample(n=min(n_per_class, len(class_df)))\n",
    "        \n",
    "        for _, row in samples.iterrows():\n",
    "            examples.append({\n",
    "                'source': format_input_with_context(row, df, use_features=False, use_context=USE_CTX),\n",
    "                'label': label.upper()\n",
    "            })\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompting(model, df, mode=\"zero-shot\", examples=None, sys_prompt_override=None, verbose=True):\n",
    "    # set output params\n",
    "    if mode == \"cot\":\n",
    "        output_type = COT_REGEX\n",
    "        max_new_tokens = MAX_COT_TOKENS\n",
    "    else:\n",
    "        output_type = StanceLabel\n",
    "        max_new_tokens = 10\n",
    "        \n",
    "    # check mode / get correct sys prompt for messages\n",
    "    if sys_prompt_override is not None: # for ablations\n",
    "        active_sys_prompt = sys_prompt_override\n",
    "    elif mode == \"cot\":\n",
    "        active_sys_prompt = COT_SYS_PROMPT\n",
    "    elif mode in (\"zero-shot\", \"few-shot\"):\n",
    "        active_sys_prompt = SYS_PROMPT # standard\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognised mode: {mode}. Choose from 'zero-shot', 'few-shot', or 'cot'.\")\n",
    "    \n",
    "    # get all prompts before\n",
    "    all_prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        input_text = format_input_with_context(row, df, use_features=False, use_context=USE_CTX)\n",
    "        messages = build_messages(input_text, examples=examples, sys_prompt=active_sys_prompt)\n",
    "        \n",
    "#         if mode == 'cot':\n",
    "#             messages[-1]['content'] += \"\\nLet's think step-by-step.\" # for cot\n",
    "        \n",
    "        # get prompt string from messages\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        all_prompts.append(prompt)\n",
    "    \n",
    "    # get responses\n",
    "    raw_responses = []\n",
    "    predictions = []\n",
    "    \n",
    "    for prompt in tqdm(all_prompts, desc=f\"Evaluating ({mode})\"):\n",
    "        response = model(prompt, output_type, max_new_tokens=max_new_tokens)\n",
    "        raw_responses.append(response)\n",
    "        \n",
    "        if mode == \"cot\":\n",
    "            pred = parse_cot_label(response) # may return None\n",
    "        else:\n",
    "            pred = response.lower() # outlines fixes to a valid stance\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # parsing errors (only for cot)\n",
    "    if mode == 'cot':\n",
    "        error_idxs = [i for i, p in enumerate(predictions) if p is None]\n",
    "        if error_idxs and verbose:\n",
    "            print(f\"Warning: {len(error_idxs)} errors in predictions. Replacing with 'comment'.\")\n",
    "            for idx in error_idxs[:5]:  # Show first 5 errors only\n",
    "                print(f\"Index: {idx}\\nRaw Model Output: '{raw_responses[idx]}'\\n\" + \"-\" * 30)\n",
    "\n",
    "        # set errors to comment (majority class)\n",
    "        predictions = [p if p is not None else 'comment' for p in predictions]\n",
    "        \n",
    "    true_labels = df['label_text'].tolist()\n",
    "    \n",
    "    # metrics\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    per_class_f1 = f1_score(true_labels, predictions, average=None, labels=labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\\nResults ({mode})\\n{'='*60}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\\n\\nPer-class F1:\")\n",
    "        for lbl, f1 in zip(labels, per_class_f1):\n",
    "            print(f\"  {lbl}: {f1:.4f}\")\n",
    "        print(f\"\\n{classification_report(true_labels, predictions, labels=labels, zero_division=0.0)}\")\n",
    "    \n",
    "    # save and return predictions\n",
    "    results_df = pd.DataFrame({'true_label': true_labels, 'predicted': predictions})\n",
    "    csv_path = f\"{RESULTS_DIR}predictions_{mode}.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    if verbose:\n",
    "        print(f\"Saved predictions: {csv_path}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'raw_responses': raw_responses,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': dict(zip(labels, per_class_f1)),\n",
    "        'confusion_matrix': confusion_matrix(true_labels, predictions, labels=labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0df239629e4ea4af843dec49837d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (zero-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# zero-shot\n",
    "zero_results = evaluate_prompting(model, use_df, mode=\"zero-shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2355ffb1a9764d468ec9616b625e3b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Isolated Ablation:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb064fc6da0a472da70d5110dd30ceb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (zero-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  minimal: 0.1321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f359f98e66484cd59e6f5d323596d18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (zero-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fig\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Run isolated ablation (each component individually)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m isolated_results = \u001b[43mrun_ablation_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mISOLATED_CONFIGS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIsolated Ablation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m plot_ablation(isolated_results, \u001b[33m'\u001b[39m\u001b[33mSystem Prompt Ablation: Isolated Component Contribution\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m    122\u001b[39m               save_path=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRESULTS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mablation_isolated.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# Run cumulative ablation (stacking components)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mrun_ablation_study\u001b[39m\u001b[34m(model, df, configs, desc)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m config_name, component_keys \u001b[38;5;129;01min\u001b[39;00m tqdm(configs.items(), desc=desc):\n\u001b[32m     54\u001b[39m     sys_prompt = build_ablation_sys_prompt(component_keys)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     ablation_results = \u001b[43mevaluate_prompting\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzero-shot\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43msys_prompt_override\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     results[config_name] = ablation_results[\u001b[33m'\u001b[39m\u001b[33mmacro_f1\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mablation_results[\u001b[33m'\u001b[39m\u001b[33mmacro_f1\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mevaluate_prompting\u001b[39m\u001b[34m(model, df, mode, examples, sys_prompt_override, verbose)\u001b[39m\n\u001b[32m     32\u001b[39m predictions = []\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m tqdm(all_prompts, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluating (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     response = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     raw_responses.append(response)\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mcot\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/outlines/models/base.py:122\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, model_input, output_type, backend, **inference_kwargs)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call the model.\u001b[39;00m\n\u001b[32m     88\u001b[39m \n\u001b[32m     89\u001b[39m \u001b[33;03mUsers can call the model directly, in which case we will create a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    118\u001b[39m \n\u001b[32m    119\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moutlines\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Generator\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/outlines/generator.py:297\u001b[39m, in \u001b[36mSteerableGenerator.__call__\u001b[39m\u001b[34m(self, prompt, **inference_kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.logits_processor.reset()\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/outlines/models/transformers.py:336\u001b[39m, in \u001b[36mTransformers.generate\u001b[39m\u001b[34m(self, model_input, output_type, **inference_kwargs)\u001b[39m\n\u001b[32m    333\u001b[39m prompts, inputs = \u001b[38;5;28mself\u001b[39m._prepare_model_inputs(model_input, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    334\u001b[39m logits_processor = \u001b[38;5;28mself\u001b[39m.type_adapter.format_output_type(output_type)\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m generated_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_output_seq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[38;5;66;03m# required for multi-modal models that return a 2D tensor even when\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# num_return_sequences is 1\u001b[39;00m\n\u001b[32m    345\u001b[39m num_samples = inference_kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mnum_return_sequences\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/outlines/models/transformers.py:385\u001b[39m, in \u001b[36mTransformers._generate_output_seq\u001b[39m\u001b[34m(self, prompts, inputs, **inference_kwargs)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_output_seq\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts, inputs, **inference_kwargs):\n\u001b[32m    383\u001b[39m     input_ids = inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     output_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minference_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# encoder-decoder returns output_ids only, decoder-only returns full seq ids\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.config.is_encoder_decoder:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    441\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    442\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    444\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    471\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    407\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    408\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    409\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    306\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[32m    307\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_attention_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    310\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv312/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:66\u001b[39m, in \u001b[36mLlamaRMSNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m     64\u001b[39m hidden_states = hidden_states.to(torch.float32)\n\u001b[32m     65\u001b[39m variance = hidden_states.pow(\u001b[32m2\u001b[39m).mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m hidden_states = hidden_states * \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariance\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvariance_epsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight * hidden_states.to(input_dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Component dictionary for easy reference (no OUTPUT_FORMAT - only used for CoT)\n",
    "PROMPT_COMPONENTS = {\n",
    "    'persona': PERSONA,\n",
    "    'instruction': INSTRUCTION,\n",
    "    'input_format': INPUT_FORMAT,\n",
    "    'label_defns': LABEL_DEFNS,\n",
    "}\n",
    "\n",
    "# Build colour palette: light grey for minimal, seaborn default for components, black for full\n",
    "_default_palette = sns.color_palette()\n",
    "COMPONENT_COLOURS = {\n",
    "    'minimal': '#D3D3D3',\n",
    "    'persona': _default_palette[0],\n",
    "    'instruction': _default_palette[1],\n",
    "    'input_format': _default_palette[2],\n",
    "    'label_defns': _default_palette[3],\n",
    "    'full': '#000000',\n",
    "}\n",
    "\n",
    "# ISOLATED: minimal + ONE component (shows individual contribution)\n",
    "ISOLATED_CONFIGS = {\n",
    "    'minimal': [],\n",
    "    '+persona': ['persona'],\n",
    "    '+instruction': ['instruction'],\n",
    "    '+input_format': ['input_format'],\n",
    "    '+label_defns': ['label_defns'],\n",
    "    'full': ['persona', 'instruction', 'input_format', 'label_defns'],\n",
    "}\n",
    "\n",
    "# CUMULATIVE: stacking components one by one\n",
    "CUMULATIVE_CONFIGS = {\n",
    "    'minimal': [],\n",
    "    '+persona': ['persona'],\n",
    "    '+instruction': ['persona', 'instruction'],\n",
    "    '+input_format': ['persona', 'instruction', 'input_format'],\n",
    "    '+label_defns (full)': ['persona', 'instruction', 'input_format', 'label_defns'],\n",
    "}\n",
    "\n",
    "\n",
    "def build_ablation_sys_prompt(component_keys):\n",
    "    \"\"\"Build system prompt from list of component keys.\"\"\"\n",
    "    if not component_keys:\n",
    "        return \"\"  # Empty system prompt for minimal\n",
    "    return \"\\n\".join(PROMPT_COMPONENTS[k] for k in component_keys)\n",
    "\n",
    "\n",
    "def run_ablation_study(model, df, configs, desc=\"Ablation\"):\n",
    "    \"\"\"Run ablation study with given configurations. Returns dict of {config_name: macro_f1}.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for config_name, component_keys in tqdm(configs.items(), desc=desc):\n",
    "        sys_prompt = build_ablation_sys_prompt(component_keys)\n",
    "        ablation_results = evaluate_prompting(\n",
    "            model, df, mode=\"zero-shot\", examples=None, \n",
    "            sys_prompt_override=sys_prompt, verbose=False\n",
    "        )\n",
    "        results[config_name] = ablation_results['macro_f1']\n",
    "        print(f\"  {config_name}: {ablation_results['macro_f1']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _get_ablation_color(config_name):\n",
    "    \"\"\"Get bar color based on config name.\"\"\"\n",
    "    if config_name == 'minimal':\n",
    "        return COMPONENT_COLOURS['minimal']\n",
    "    elif 'full' in config_name:\n",
    "        return COMPONENT_COLOURS['full']\n",
    "    for comp in COMPONENT_COLOURS:\n",
    "        if comp in config_name:\n",
    "            return COMPONENT_COLOURS[comp]\n",
    "    return COMPONENT_COLOURS['minimal']\n",
    "\n",
    "\n",
    "def plot_ablation(results, title, save_path=None, show_delta=False):\n",
    "    \"\"\"Plot horizontal bar chart for ablation results.\"\"\"\n",
    "    df_plot = pd.DataFrame({\n",
    "        'config': list(results.keys()),\n",
    "        'score': list(results.values())\n",
    "    })\n",
    "    df_plot['color'] = [_get_ablation_color(c) for c in df_plot['config']]\n",
    "    palette = dict(zip(df_plot['config'], df_plot['color']))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.barplot(data=df_plot, y='config', x='score', hue='config', palette=palette, \n",
    "                ax=ax, edgecolor='white', legend=False)\n",
    "    \n",
    "    ax.set_xlabel('Macro F1 Score')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_title(f'{title}\\n(Zero-Shot on Dev Set)')\n",
    "    ax.set_xlim(0, df_plot['score'].max() * 1.15)\n",
    "    \n",
    "    for i, (score, config) in enumerate(zip(df_plot['score'], df_plot['config'])):\n",
    "        ax.text(score + 0.005, i, f'{score:.3f}', va='center', fontsize=10)\n",
    "    \n",
    "    if 'minimal' in results:\n",
    "        ax.axvline(x=results['minimal'], color=COMPONENT_COLOURS['minimal'], \n",
    "                   linestyle='--', alpha=0.7, linewidth=1.5, label='Minimal baseline')\n",
    "        ax.legend(loc='lower right')\n",
    "    \n",
    "    if show_delta:\n",
    "        scores = list(results.values())\n",
    "        for i in range(1, len(scores)):\n",
    "            delta = scores[i] - scores[i-1]\n",
    "            sign = '+' if delta >= 0 else ''\n",
    "            ax.text(scores[i] + 0.04, i - 0.25, f'{sign}{delta:.3f}', \n",
    "                    fontsize=9, color='dimgray', style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Run isolated ablation (each component individually)\n",
    "isolated_results = run_ablation_study(model, use_df, ISOLATED_CONFIGS, desc=\"Isolated Ablation\")\n",
    "plot_ablation(isolated_results, 'System Prompt Ablation: Isolated Component Contribution', \n",
    "              save_path=f\"{RESULTS_DIR}ablation_isolated.png\")\n",
    "\n",
    "# Run cumulative ablation (stacking components)\n",
    "cumulative_results = run_ablation_study(model, use_df, CUMULATIVE_CONFIGS, desc=\"Cumulative Ablation\")\n",
    "plot_ablation(cumulative_results, 'System Prompt Ablation: Cumulative Component Stacking', \n",
    "              save_path=f\"{RESULTS_DIR}ablation_cumulative.png\", show_delta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# few-shot\n",
    "few_shot_examples = get_few_shot_examples(train_df, n_per_class=1)\n",
    "few_results = evaluate_prompting(model, use_df, mode=\"few-shot\", examples=few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0da05545f546d4a7575e9406d1b106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (cot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 115 errors in predictions. Replacing with 'comment'.\n",
      "Index: 1\n",
      "Raw Model Output: 'Step 1: Identify the claim in the [SRC] tweet - a plane crash in the French Alps has occurred, which is a verifiable event that likely happened true (no need to verify the details of the crash itself, just its occurrence is true) or at least not false (given the information provided in the [SRC] tweet). The claim is likely true. The tone of the [SRC] tweet is somber, but the language used does not directly deny or question the occurrence of'\n",
      "------------------------------\n",
      "Index: 5\n",
      "Raw Model Output: 'Step 1: Analyze the language used in the TARGET tweet. The user is expressing sadness and shock, but also explicitly states that it was a deliberate suicide, which is a fact confirmed by the news source mentioned in the SRC tweet (Germanwings Airbus A320 crash in French Alps). This implies that the TARGET is not questioning the veracity of the event, but rather adding more information to the story that is already known to be true by the general public and the news source mentioned in the'\n",
      "------------------------------\n",
      "Index: 6\n",
      "Raw Model Output: 'Step 1: Identify the topic of the source claim (SRC) - a plane crash in the French Alps, which is a tragic event and not related to the number of Nazis killed by Leebowitz (likely a reference to Adolf Hitler's deputy Rudolf Hess, but not relevant to the plane crash claim). The source claim is factual and verifiable, likely true, but not relevant to the TARGET reply. However, the TARGET reply starts with a phrase that could be seen as insensitive,'\n",
      "------------------------------\n",
      "Index: 7\n",
      "Raw Model Output: 'Here's the step-by-step analysis for the task: The reply is expressing sympathy and suggesting that some passengers might have survived, which is a form of questioning the initial report's accuracy. It's not directly denying or confirming the veracity of the source claim, but rather adding a nuance. However, since it does not directly contradict the claim, but rather adds a conditional statement, it is leaning towards supporting the initial claim. The reply is not simply a denial or a query, but rather an'\n",
      "------------------------------\n",
      "Index: 8\n",
      "Raw Model Output: 'Step 1: [TARGET] is expressing skepticism and requesting further information from the NTSB before accepting the cause of the crash as terrorism, indicating a questioning of the source claim's veracity or implications of the rumour. This suggests a need for more evidence or clarification before making a judgment. Step 2: This response is asking for more information to support or refute the source claim, rather than directly accepting or denying it. Step 3: The reply does not directly support, deny,'\n",
      "------------------------------\n",
      "\n",
      "============================================================\n",
      "Results (cot)\n",
      "============================================================\n",
      "Macro F1: 0.3673\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.2909\n",
      "  deny: 0.2143\n",
      "  query: 0.3542\n",
      "  comment: 0.6098\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.39      0.23      0.29        69\n",
      "        deny       0.18      0.27      0.21        11\n",
      "       query       0.25      0.61      0.35        28\n",
      "     comment       0.65      0.58      0.61       173\n",
      "\n",
      "    accuracy                           0.48       281\n",
      "   macro avg       0.37      0.42      0.37       281\n",
      "weighted avg       0.52      0.48      0.49       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_cot.csv\n"
     ]
    }
   ],
   "source": [
    "# CoT prompting\n",
    "\n",
    "cot_results = evaluate_prompting(model, use_df, mode=\"cot\", examples=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 02 Jan 2026\n",
      "\n",
      "You are an expert in rumour stance analysis on twitter.\n",
      "Your task is to classify the stance of the [TARGET] tweet towards the veracity of the rumour in the [SRC] tweet.\n",
      "The input will be provided as a single string containing labeled segments:\n",
      "\"[SRC] ... [PARENT] ... [TARGET] ...\". (Note: [PARENT] is omitted if [TARGET] replies directly to [SRC])\n",
      "\n",
      "Classification Labels:\n",
      "- SUPPORT: The reply explicitly supports or provides evidence for the veracity of the source claim\n",
      "- DENY: The tweet explicitly denies or provides counter-evidence for the veracity of the source claim\n",
      "- QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
      "- COMMENT: The reply makes their own opinionated/neutral comment without a clear contribution to assessing the veracity of the source claim\n",
      "\n",
      "Your response should follow this EXACT format:\n",
      "Reasoning: [VERY BRIEFLY explain why the label fits. Use a two stage strategy: Stage 1 classify Stance vs Non-Stance - If Non-stance, skip stage 2 and answer 'COMMENT'. Otherwise: Stage 2 classify Support / Deny / Query]\n",
      "Label: [EXACTLY one of: SUPPORT, DENY, QUERY, or COMMENT]<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Text: [SRC] Heart goes out to 148 passengers and crew of Germanwings Airbus A320 that has crashed in French Alps , Southern France HTTPURL [TARGET] @USER So sad to know that it was a deliberate suicide\n",
      "\n",
      "Classify the stance of [TARGET] towards the veracity of the rumour in [SRC]:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "Response: Reasoning: Stage 1: Stance vs Non-Stance - The reply directly responds to the source claim, indicating a stance. Stage 2: Support / Deny / Query - The reply does not provide evidence or ask for evidence, but instead makes a judgmental comment. It explicitly states that the crash was a deliberate suicide, which can be seen as a statement of fact that may or may not be true. However, given the context, it is more likely that [TARGET] is expressing sympathy and agreeing with the source claim, rather than providing objective evidence. Therefore, the label is SUPPORT, as [TARGET] is implicitly endorsing the veracity of the source claim. Label: SUPPORT\n",
      "\n",
      "Prediction: support\n",
      "True label: comment\n"
     ]
    }
   ],
   "source": [
    "# test a single eg\n",
    "row = dev_df.iloc[5]\n",
    "input_text = format_input_with_context(row, dev_df, use_features=False, use_context=USE_CTX)\n",
    "messages = build_messages(input_text, None, COT_SYS_PROMPT) \n",
    "# messages[-1]['content'] += \"\\nLet's think step-by-step.\" # for cot\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "response = model(prompt, COT_REGEX, max_new_tokens=MAX_COT_TOKENS)\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "print(f\"Prediction: {parse_cot_label(response)}\")\n",
    "print(f\"True label: {row['label_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot cot\n",
    "cot_few_shot_examples = get_cot_examples(train_df)\n",
    "cot_few_results = evaluate_prompting(model, use_df, mode=\"cot\", examples=cot_few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312k",
   "language": "python",
   "name": "venv312k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
