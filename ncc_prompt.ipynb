{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import outlines # see https://dottxt-ai.github.io/outlines/latest/features/models/transformers/\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "from itertools import combinations\n",
    "\n",
    "from data_loader import load_dataset, format_input_with_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from saved_data/datasets.pkl...\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "USE_CTX = False # whether to use [CTX] parts of threads (too slow for True)\n",
    "MAX_COT_TOKENS = 200 # max n tokens for cot response\n",
    "VALID_STANCES = ['SUPPORT', 'DENY', 'QUERY', 'COMMENT']\n",
    "RELOAD_MODEL = False # takes up lots of mem if we keep reloading\n",
    "\n",
    "RESULTS_DIR = \"./results/prompting/\"\n",
    "RANDOM_SEED = 42\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# load data\n",
    "train_df, dev_df, test_df = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76158864033b4fa58f40c098c1549979",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "# output types for outlines\n",
    "StanceLabel = Literal[\"SUPPORT\", \"DENY\", \"QUERY\", \"COMMENT\"]\n",
    "COT_REGEX = outlines.types.Regex(r\"[\\s\\S]*Label: (SUPPORT|DENY|QUERY|COMMENT)\")\n",
    "\n",
    "if 'model' not in globals() or RELOAD_MODEL:\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # use outlines\n",
    "    model = outlines.from_transformers(hf_model, tokenizer)\n",
    "    \n",
    "    # get rid of annoying warnings re pad_token\n",
    "    model.tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.tokenizer.padding_side = \"left\"\n",
    "    model.model.generation_config.pad_token_id = model.tokenizer.pad_token_id\n",
    "\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "# see https://huggingface.co/docs/transformers/en/tasks/prompting\n",
    "# https://www.promptingguide.ai/techniques/zeroshot\n",
    "\n",
    "PERSONA = \"\"\"You are an expert in rumour stance analysis on twitter.\"\"\"\n",
    "INSTRUCTION = \"\"\"Your task is to classify the stance of the [TARGET] tweet towards the veracity of the rumour in the [SRC] tweet.\"\"\"\n",
    "INPUT_FORMAT = \"\"\"\\\n",
    "The input will be provided as a single string containing labeled segments:\n",
    "\"[SRC] ... [PARENT] ... [TARGET] ...\". (Note: [PARENT] is omitted if [TARGET] replies directly to [SRC])\n",
    "\"\"\"\n",
    "LABEL_DEFNS = \"\"\"\\\n",
    "Classification Labels:\n",
    "- SUPPORT: The reply explicitly supports or provides evidence for the veracity of the source claim\n",
    "- DENY: The tweet explicitly denies or provides counter-evidence for the veracity of the source claim\n",
    "- QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
    "- COMMENT: The reply makes their own opinionated/neutral comment without a clear contribution to assessing the veracity of the source claim\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "\"\"\"\n",
    "# output format set by outlines\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Text: {thread_context}\n",
    "\n",
    "Classify the stance of [TARGET] towards the veracity of the rumour in [SRC]:\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(thread_context):\n",
    "    return USER_PROMPT_TEMPLATE.format(thread_context=thread_context)\n",
    "\n",
    "\n",
    "def build_messages(thread_context, examples=None, sys_prompt=None):\n",
    "    init_prompt = sys_prompt if sys_prompt is not None else SYS_PROMPT # allow custom sys prompt (eg. for CoT/ablations)\n",
    "    messages = [{\"role\": \"system\", \"content\": init_prompt}]\n",
    "    if examples: # for few-shot\n",
    "        for ex in examples:\n",
    "            messages.append({\"role\": \"user\", \"content\": build_user_prompt(ex['source'])})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ex['response']})\n",
    "    messages.append({\"role\": \"user\", \"content\": build_user_prompt(thread_context)}) # final user prompt to answer\n",
    "    return messages\n",
    "\n",
    "def build_ablation_sys_prompt(component_keys): # ablated sys prompt\n",
    "    if not component_keys:\n",
    "        return \"\"\n",
    "    return \"\\n\".join(PROMPT_COMPONENTS[k] for k in component_keys)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COT prompts \n",
    "\n",
    "# add output format to think step by step\n",
    "COT_OUTPUT_FORMAT = \"\"\"\\\n",
    "Your response should follow this EXACT format:\n",
    "Reasoning: [VERY BRIEFLY explain why the label fits. Use a two stage strategy: Stage 1 classify Stance vs Non-Stance - If Non-stance, skip stage 2 and answer 'COMMENT'. Otherwise: Stage 2 classify Support / Deny / Query]\n",
    "Label: [EXACTLY one of: SUPPORT, DENY, QUERY, or COMMENT]\n",
    "\"\"\"\n",
    "\n",
    "COT_SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "{COT_OUTPUT_FORMAT}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# for few-shot cot\n",
    "\n",
    "# handselected ids for reasoning\n",
    "COT_FEW_SHOT_IDS = {\n",
    "    'support': '524967134339022848', # @TheKirkness Radio Canada tweeting same. must be true :-(\n",
    "    'deny': '544292581950357504', # @JSchoenberger7 @TheAnonMessage not an Isis flag. Just an Islamic one. Stop spreading false rumors.\n",
    "    'query': '544281192632053761', # @mscott Are your reporters 100% sure it is an ISIS flag? Cause that is what is being reported. #facts\n",
    "    'comment': '552804023389392896', # @thei100 @Independent these fuckers thinking its a GTA heist mission\n",
    "}\n",
    "\n",
    "COT_EXAMPLES = {\n",
    "    'support': \"1. Stance: takes position on veracity\\n2. \\\"must be true\\\" → affirms claim\\nLabel: SUPPORT\",\n",
    "    'deny': \"1. Stance: challenges veracity\\n2. \\\"not an Isis flag\\\", \\\"false rumors\\\" → rejects claim\\nLabel: DENY\",\n",
    "    'query': \"1. Stance: engages with veracity\\n2. \\\"Are your reporters 100% sure?\\\" → requests evidence\\nLabel: QUERY\",\n",
    "    'comment': \"1. Stance: no veracity assessment\\n2. Offers opinion/reaction only\\nLabel: COMMENT\",\n",
    "}\n",
    "\n",
    "def get_cot_examples(train_df):\n",
    "    examples = []\n",
    "    for label, tweet_id in COT_FEW_SHOT_IDS.items():\n",
    "        matches = train_df[train_df['tweet_id'] == tweet_id]\n",
    "        if len(matches) == 0:\n",
    "            print(f\"Warning: COT example tweet_id {tweet_id} not found\")\n",
    "            continue\n",
    "        row = matches.iloc[0]\n",
    "        examples.append({\n",
    "            'source': format_input_with_context(row, train_df, use_features=False, use_context=USE_CTX),\n",
    "            'response': COT_EXAMPLES[label]\n",
    "        })\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cot_label(text): # get final label from cot response\n",
    "    match = re.search(r'Label:\\s*(SUPPORT|DENY|QUERY|COMMENT)', text, re.IGNORECASE)\n",
    "    return match.group(1).lower() if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LLM --\n",
    "def plot_confusion_matrix(cm, mode, save_path=None):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix ({mode})')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_results(results, model_key, mode):\n",
    "    \"\"\"Save evaluation results to JSON.\"\"\"\n",
    "    output = {\n",
    "        'model': model_key,\n",
    "        'mode': mode,\n",
    "        'macro_f1': results['macro_f1'],\n",
    "        'per_class_f1': results['per_class_f1'],\n",
    "        'predictions': results['predictions'],\n",
    "        'true_labels': results['true_labels'],\n",
    "        'raw_responses': results['raw_responses']\n",
    "    }\n",
    "    filename = f\"{RESULTS_DIR}{model_key}_{mode}_results.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot examples\n",
    "\n",
    "# from different sources and topics\n",
    "DIVERSE_FEW_SHOT_IDS = {\n",
    "    'support': '553486439129038848', # 'BREAKING: French police tell AP suspects in #CharlieHebdo attack have taken a hostage northeast of Paris.'\n",
    "    'deny': '500384255814299648', # '@TPM @McBlondeLand Awful! The DOJ needs to do thorough investigation of this PD-these problems are systemic. The citizens deserve better!'\n",
    "    'query': '525003827494531073', # '@CTVNews @ctvsaskatoon so what happened at Rideau? Nothing?'\n",
    "    'comment': '544314008980172800', # \"@WilliamsJon @morgfair how do we know the guy isn't just pissed that he got fired or foreclosed on. Or his gf is cheating on him.\"\n",
    "}\n",
    "    \n",
    "# From same source thread\n",
    "SAME_SRC_SOURCE_ID = '529739968470867968' # 'Prince not playing Massey Hall tonight, promoter says http://t.co/clVHvM07kx http://t.co/IADirsFOOC'\n",
    "SAME_SRC_FEW_SHOT_IDS = {\n",
    "    'support': '529740822238232577', # 'athenaph what tease!!! RT @CBCNews: Prince not playing Massey Hall tonight, promoter says http://t.co/245JKmwWHH'\n",
    "    'deny': '529740809672077313', # '.@CBCNews I can also confirm he will not be playing in Rankin Inlet, NU either'\n",
    "    'query': '529748991849013248', # '@CBCNews And, ... ?\\nWhen @CBCNews asked Live Nation why no one made this clarification 4 or 5 hours earlier?'\n",
    "    'comment': '529741574742507520', # '@CBCNews  lol.'\n",
    "}\n",
    "\n",
    "# random stratified selection (one per stance but randomly picked)\n",
    "RANDOM_STRATIFIED_FEW_SHOT_IDS = {\n",
    "    'support': '544306719686656000', # 'US evacuated Consulate in Sydney after #sydneysiege. Emergency warning to U.S. citizens urging them to \"maintain high level of vigilance\".'\n",
    "    'deny': '524990163446140928', # 'Police have clarified that there were two shootings in Ottawa today, not three: at the War Memorial and Parliament Hill.'\n",
    "    'query': '500386447158161408', # '@TPM Wow. I thought a Tweep was being sarcastic, but...it really happened that way?!?'\n",
    "    'comment': '553543395717550080', # '@FoxNews @Machma7Machos the terrorists need empathy send them to Harvard University'\n",
    "}\n",
    "\n",
    "FEW_SHOT_SETS = {\n",
    "    'diverse': DIVERSE_FEW_SHOT_IDS,\n",
    "    'same_src': SAME_SRC_FEW_SHOT_IDS,\n",
    "    'random': RANDOM_STRATIFIED_FEW_SHOT_IDS,\n",
    "}\n",
    "\n",
    "\n",
    "def get_few_shot_examples(df, n_per_class=1, use_set=None, classes=None):\n",
    "    if classes is None:\n",
    "        classes = ['support', 'deny', 'query', 'comment']\n",
    "    examples = []\n",
    "    \n",
    "    if use_set is not None:\n",
    "        # deterministic using defined sets\n",
    "        if use_set not in FEW_SHOT_SETS:\n",
    "            raise ValueError(f\"Unknown use_set '{use_set}'. Choose from: {list(FEW_SHOT_SETS.keys())}\")\n",
    "        \n",
    "        id_dict = FEW_SHOT_SETS[use_set]\n",
    "        for label in classes:\n",
    "            tweet_id = id_dict.get(label)\n",
    "            if tweet_id is None:\n",
    "                continue\n",
    "            matches = df[df['tweet_id'] == tweet_id]\n",
    "            if len(matches) == 0:\n",
    "                print(f\"Warning: Few-shot example tweet_id {tweet_id} not found in df\")\n",
    "                continue\n",
    "            row = matches.iloc[0]\n",
    "            examples.append({\n",
    "                'source': format_input_with_context(row, df, use_features=False, use_context=USE_CTX),\n",
    "                'response': label.upper()\n",
    "            })\n",
    "    else:\n",
    "        # Non-deterministic: random stratified sampling\n",
    "        for label in classes:\n",
    "            class_df = df[df['label_text'] == label]\n",
    "            samples = class_df.sample(n=min(n_per_class, len(class_df)))\n",
    "            \n",
    "            for _, row in samples.iterrows():\n",
    "                examples.append({\n",
    "                    'source': format_input_with_context(row, df, use_features=False, use_context=USE_CTX),\n",
    "                    'response': label.upper()\n",
    "                })\n",
    "    \n",
    "    return examples\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompting(model, df, mode=\"zero-shot\", examples=None, sys_prompt_override=None, verbose=True):\n",
    "    # mode can be zero-shot or cot (for few-shot, just pass list into examples)\n",
    "    # sys_prompt_override is just for ablations (cot sys prompt added automatically)\n",
    "\n",
    "    # set output params\n",
    "    if mode == \"cot\":\n",
    "        output_type = COT_REGEX\n",
    "        max_new_tokens = MAX_COT_TOKENS\n",
    "    else:\n",
    "        output_type = StanceLabel\n",
    "        max_new_tokens = 10\n",
    "    \n",
    "    # allow custom sys prompt for ablations & cot\n",
    "    if sys_prompt_override is not None:\n",
    "        active_sys_prompt = sys_prompt_override\n",
    "    elif mode == \"cot\":\n",
    "        active_sys_prompt = COT_SYS_PROMPT\n",
    "    else:\n",
    "        active_sys_prompt = SYS_PROMPT\n",
    "    \n",
    "    predictions = []\n",
    "    raw_responses = []\n",
    "    error_count = 0 # track cot errors where it doesn't return label\n",
    "    err_ids = []\n",
    "    err_responses = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=f\"Eval ({mode})\"):\n",
    "        input_text = format_input_with_context(row, df, use_features=False, use_context=USE_CTX) # format with [SRC] etc\n",
    "        messages = build_messages(input_text, examples=examples, sys_prompt=active_sys_prompt) # get messages dict\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) # get prompt string\n",
    "        response = model(prompt, output_type, max_new_tokens=max_new_tokens) # get response\n",
    "        raw_responses.append(response)\n",
    "\n",
    "        if mode == \"cot\":\n",
    "            pred = parse_cot_label(response) # may return none if didn't give label\n",
    "            if pred is None:\n",
    "                error_count += 1\n",
    "                err_ids.append(row['tweet_id'])\n",
    "                err_responses.append(response)\n",
    "                pred = 'comment' # so default to 'comment' as majority class\n",
    "        else:\n",
    "            pred = response.lower().strip()\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    true_labels = df['label_text'].tolist()\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    per_class_f1 = f1_score(true_labels, predictions, average=None, labels=labels)\n",
    "\n",
    "    if verbose:\n",
    "        if error_count > 0:\n",
    "            print(f\"{error_count} ERRORS\")\n",
    "            for i in range(error_count):\n",
    "                    print(f\"Tweet id: {err_ids[i]}\\nResponse: '{err_responses[i]}'\\n\" + \"-\" * 30)\n",
    "                    if i > 5:\n",
    "                        break # only show 1st 5\n",
    "        print(f\"\\n{classification_report(true_labels, predictions, labels=labels, zero_division=0.0)}\")\n",
    "    \n",
    "    return {\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': dict(zip(labels, per_class_f1)),\n",
    "        'confusion_matrix': confusion_matrix(true_labels, predictions, labels=labels),\n",
    "        'predictions_vs_responses': list(zip(predictions, raw_responses)),\n",
    "        'error_ids_vs_responses': list(zip(err_ids, err_responses)) if error_count > 0 else None\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0df239629e4ea4af843dec49837d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (zero-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results (zero-shot)\n",
      "============================================================\n",
      "Macro F1: 0.1008\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.0000\n",
      "  deny: 0.1714\n",
      "  query: 0.2204\n",
      "  comment: 0.0115\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.00      0.00      0.00        69\n",
      "        deny       0.10      0.55      0.17        11\n",
      "       query       0.12      0.96      0.22        28\n",
      "     comment       1.00      0.01      0.01       173\n",
      "\n",
      "    accuracy                           0.12       281\n",
      "   macro avg       0.31      0.38      0.10       281\n",
      "weighted avg       0.63      0.12      0.04       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_zero-shot.csv\n"
     ]
    }
   ],
   "source": [
    "# zero-shot\n",
    "zero_results = evaluate_prompting(model, test_df, mode=\"zero-shot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cde76a0d87949a399aaaf8a022e014e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (few-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results (few-shot)\n",
      "============================================================\n",
      "Macro F1: 0.3789\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.3977\n",
      "  deny: 0.2963\n",
      "  query: 0.4065\n",
      "  comment: 0.4153\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.33      0.51      0.40        69\n",
      "        deny       0.25      0.36      0.30        11\n",
      "       query       0.26      0.89      0.41        28\n",
      "     comment       0.78      0.28      0.42       173\n",
      "\n",
      "    accuracy                           0.40       281\n",
      "   macro avg       0.40      0.51      0.38       281\n",
      "weighted avg       0.60      0.40      0.41       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_few-shot.csv\n"
     ]
    }
   ],
   "source": [
    "# few-shot\n",
    "few_shot_examples = get_few_shot_examples(train_df, n_per_class=1)\n",
    "few_results = evaluate_prompting(model, test_df, mode=\"few-shot\", examples=few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f222a9d6c1514600b7847a826937ffda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (cot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 3 errors in predictions. Replacing with 'comment'.\n",
      "Index: 65\n",
      "Raw Model Output: 'Reasoning: Stage 1: Stance vs Non-Stance - [TARGET] is responding to the source claim, so it is a stance. Stage 2: Support / Deny / Query - The reply does not explicitly support or deny the veracity of the source claim, nor does it ask for additional evidence. Instead, it makes a comment, \"another great tragedy\", which is an opinionated statement. Therefore, the label is: COMMENT. [COMMENT] The reply is a neutral comment without a clear contribution to assessing the veracity of the source claim. It does not provide any direct support, denial, or query, but rather expresses sympathy or shock. The tone is also somewhat informal, which is typical of a condolence message. The reply does not directly engage with the veracity of the claim, but rather reacts to the news. Therefore, the reply does not fit into the categories of Support, Deny, or Query. A more fitting response might have been \"Did'\n",
      "------------------------------\n",
      "Index: 95\n",
      "Raw Model Output: 'Reasoning: Stage 1: Non-Stance - [TARGET] does not directly respond to the veracity of the claim in [SRC], but rather provides additional information that appears to contradict the number of passengers on board. This suggests a neutral or informative tone rather than a stance on the veracity of the claim. Stage 2: CLASSIFY: COMMENT - [TARGET] provides new information that does not directly support, deny, or query the original claim, but rather offers an alternative piece of information that could be seen as a fact-check or correction. However, given the context of the original tweet being a breaking news report, it's more likely that [TARGET] is simply providing an update or correction rather than a neutral comment. A more accurate classification would be CLASSIFY: COMMENT. But given the context, I will classify it as SUPPORT as [TARGET] is providing factual information that updates the original claim, which can be seen as supporting the veracity of the original claim. However,'\n",
      "------------------------------\n",
      "Index: 111\n",
      "Raw Model Output: 'Reasoning: Stage 1: Stance vs Non-Stance - Since [TARGET] responds directly to [SRC] and does not make a general comment, it is a stance. Stage 2: Support / Deny / Query - [TARGET] does not explicitly support or deny the claim, but rather provides an alternative explanation. It neither confirms nor denies the original claim, so it can be classified as neither SUPPORT nor DENY. The tone is more inquiring, so it is neither SUPPORT nor DENY. However, it does provide a counter-narrative to the original claim, so it is neither SUPPORT nor DENY. The best fit is QUERY, but since it provides a narrative, it is more of a COMMENT. Therefore, the label is COMMENT. However, a better fit is actually DENY, as it does provide a narrative that directly counters the original claim. So, the correct answer is DENY. The comment actually counters the original claim by providing an alternative explanation.'\n",
      "------------------------------\n",
      "\n",
      "============================================================\n",
      "Results (cot)\n",
      "============================================================\n",
      "Macro F1: 0.3777\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.3359\n",
      "  deny: 0.1176\n",
      "  query: 0.4308\n",
      "  comment: 0.6265\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.35      0.32      0.34        69\n",
      "        deny       0.09      0.18      0.12        11\n",
      "       query       0.38      0.50      0.43        28\n",
      "     comment       0.65      0.60      0.63       173\n",
      "\n",
      "    accuracy                           0.51       281\n",
      "   macro avg       0.37      0.40      0.38       281\n",
      "weighted avg       0.53      0.51      0.52       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_cot.csv\n"
     ]
    }
   ],
   "source": [
    "# CoT prompting\n",
    "\n",
    "cot_results = evaluate_prompting(model, test_df, mode=\"cot\", examples=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93714374308845c2a2ddd33b52765f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (cot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 140 errors in predictions. Replacing with 'comment'.\n",
      "Index: 4\n",
      "Raw Model Output: '1. Stance: acknowledges veracity indirectly (through shared source URL, but no direct statement on factuality) and expresses sympathy, but doesn't provide any additional information or comment on the incident's veracity itself. However, given the absence of any direct challenge, lack of denial, or query, the sentiment is sympathetic rather than informative or argumentative. It can be argued that it is a neutral comment rather than a stance on the veracity of the claim. However, in the context of your original request, a more fitting response would be to classify it as a COMMENT. Since the target does not directly address the veracity of the claim, the response does not challenge or support the claim but rather provides a sympathetic response. Therefore, a more fitting response would be to label it as COMMENT. However, a more nuanced approach might be to label it as QUERY in the sense that it seeks to understand the context of the incident, but in the context of your original request, it is more'\n",
      "------------------------------\n",
      "Index: 6\n",
      "Raw Model Output: '1. Stance: takes position on veracity (in relation to a different claim, not the main one in [SRC]) but is off-topic to [SRC] and can be disregarded as a non-sequitur for [SRC] classification. However, given the context of the [SRC] it is clear [TARGET] is making a comment about a previous [SRC] which is about plane crash. Hence [TARGET] is making a comment on a related topic. Hence it is safe to disregard [TARGET] for [SRC] classification. However in a different context it would be classified as: 2. Stance: takes position on veracity (in relation to a different claim, not the main one in [SRC]) but is off-topic to [SRC]. The comment is about a different event (Leebowitz) and can be disregarded as non-relevant to [SRC]. However in a different context it would be classified as: 3. Stance: takes'\n",
      "------------------------------\n",
      "Index: 7\n",
      "Raw Model Output: '1. Stance: assesses veracity in relation to additional information provided by [SRC] and adds new information (possibility of survivors) → takes position on veracity of original claim and provides new evidence/assumption to support or contradict it. However, the tone is more speculative than a clear assertion or denial, and doesn't explicitly support or deny the claim, but rather offers a possibility that might be relevant to the veracity of the original claim. However, the tone is more speculative and the language used is not strongly supporting or denying, rather adding a possibility. Therefore, it is slightly leaning towards QUERY but the possibility of survival is added in a somewhat speculative manner and doesn't directly address the veracity of the original claim. But the most fitting label given the context is QUERY, as the statement adds new information and doesn't strongly support or deny the original claim, but it does not explicitly ask for evidence. The statement is more of a speculative comment than a clear stance. However'\n",
      "------------------------------\n",
      "Index: 8\n",
      "Raw Model Output: '1. Stance: takes position on veracity (expressing doubt on terrorism aspect, but not directly addressing the general veracity of the claim about the crash itself) → takes position on veracity in part, but not entirely on the crash itself, so doesn't fully classify as support or deny, but more as a neutral or cautious stance on the terrorism claim specifically, however since [TARGET] doesn't directly address the crash itself but rather the aspect of terrorism, it is more aligned with a cautious stance on a specific aspect of the claim rather than the crash itself, and it is cautious, so it is better classified as a query on the terrorism claim rather than a direct comment on the crash itself, but since it doesn't ask for evidence or clarification on the crash itself, it is more aligned with a cautious stance rather than a query, and since it does express a view on the claim, it is not a comment, it is more aligned with a cautious stance, but since it is'\n",
      "------------------------------\n",
      "Index: 9\n",
      "Raw Model Output: '1. Stance: draws conclusion on veracity based on source claim, and adds information that could be seen as supporting the claim, but the added information is a generalization and not a direct link to the claim. However the claim is not explicitly supported by the added information. The added information is a generalization of the source claim, and the tone of the message is critical of the source claim.  However, the tone of the message is more critical than supportive. This makes the message slightly more than just neutral, but less than supportive. However, the added information can be seen as a generalization of the source claim. The claim is not explicitly supported by the added information. The tone of the message is critical of the source claim, and the added information is a generalization of the source claim. The tone of the message is critical of the source claim, and the added information can be seen as a generalization of the source claim. The tone of the message is critical of the'\n",
      "------------------------------\n",
      "\n",
      "============================================================\n",
      "Results (cot)\n",
      "============================================================\n",
      "Macro F1: 0.4041\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.1875\n",
      "  deny: 0.2727\n",
      "  query: 0.4364\n",
      "  comment: 0.7198\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.33      0.13      0.19        69\n",
      "        deny       0.27      0.27      0.27        11\n",
      "       query       0.44      0.43      0.44        28\n",
      "     comment       0.65      0.81      0.72       173\n",
      "\n",
      "    accuracy                           0.58       281\n",
      "   macro avg       0.42      0.41      0.40       281\n",
      "weighted avg       0.54      0.58      0.54       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_cot.csv\n"
     ]
    }
   ],
   "source": [
    "# few shot cot\n",
    "cot_few_shot_examples = get_cot_examples(train_df)\n",
    "cot_few_results = evaluate_prompting(model, test_df, mode=\"cot\", examples=cot_few_shot_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# custom palette for sys prompt ablation (exp 1)\n",
    "custom_ablation_palette = sns.color_palette()\n",
    "custom_ablation_palette = [(0.82745098, 0.82745098, 0.82745098)] + custom_ablation_palette # grey for no sys prompt\n",
    "custom_ablation_palette[5] = (0,0,0) # black for full sys prompt\n",
    "\n",
    "# helpers\n",
    "def load_or_create_csv(path, columns):\n",
    "    if os.path.exists(path):\n",
    "        return pd.read_csv(path)\n",
    "    return pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "def append_row_to_csv(path, row_dict):\n",
    "    df = pd.DataFrame([row_dict])\n",
    "    df.to_csv(path, mode='a', header=not os.path.exists(path), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys prompt ablation\n",
    "\n",
    "PROMPT_COMPONENTS = {\n",
    "    'persona': PERSONA,\n",
    "    'instruction': INSTRUCTION,\n",
    "    'input_format': INPUT_FORMAT,\n",
    "    'label_defns': LABEL_DEFNS,\n",
    "}\n",
    "\n",
    "ISOLATED_CONFIGS = {\n",
    "    'minimal': [],\n",
    "    '+persona': ['persona'],\n",
    "    '+instruction': ['instruction'],\n",
    "    '+input_format': ['input_format'],\n",
    "    '+label_defns': ['label_defns'],\n",
    "    'full': ['persona', 'instruction', 'input_format', 'label_defns'],\n",
    "}\n",
    "\n",
    "CUMULATIVE_CONFIGS = {\n",
    "    'minimal': [],\n",
    "    '+persona': ['persona'],\n",
    "    '+instruction': ['persona', 'instruction'],\n",
    "    '+input_format': ['persona', 'instruction', 'input_format'],\n",
    "    '+label_defns (full)': ['persona', 'instruction', 'input_format', 'label_defns'],\n",
    "}\n",
    "\n",
    "def run_exp1_ablation(model, tokenizer, train_df, dev_df):    \n",
    "    csv_path = f\"{RESULTS_DIR}exp1_ablation.csv\"\n",
    "    columns = ['config_type', 'config_name', 'macro_f1']\n",
    "    \n",
    "    existing_df = load_or_create_csv(csv_path, columns)\n",
    "    completed = set(zip(existing_df.get('config_type', []), existing_df.get('config_name', [])))\n",
    "    \n",
    "    all_configs = [\n",
    "        ('isolated', ISOLATED_CONFIGS),\n",
    "        ('cumulative', CUMULATIVE_CONFIGS),\n",
    "    ]\n",
    "    \n",
    "    for config_type, configs in all_configs:\n",
    "        for config_name, component_keys in configs.items():\n",
    "            if (config_type, config_name) in completed:\n",
    "                print(f\"  Skipping {config_type}/{config_name} (already done)\")\n",
    "                continue\n",
    "            \n",
    "            sys_prompt = build_ablation_sys_prompt(component_keys)\n",
    "            results = evaluate_prompting(model, dev_df, mode=\"zero-shot\", \n",
    "                                         sys_prompt_override=sys_prompt, verbose=False)\n",
    "            \n",
    "            row = {'config_type': config_type, 'config_name': config_name, 'macro_f1': results['macro_f1']}\n",
    "            append_row_to_csv(csv_path, row)\n",
    "            print(f\"  {config_type}/{config_name}: {results['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot selection strategy chart\n",
    "def run_exp2_fewshot_strategies(model, tokenizer, train_df, dev_df):    \n",
    "    csv_path = f\"{RESULTS_DIR}exp2_fewshot_strategies.csv\"\n",
    "    columns = ['strategy', 'n_per_class', 'macro_f1', 'support_f1', 'deny_f1', 'query_f1', 'comment_f1']\n",
    "    \n",
    "    existing_df = load_or_create_csv(csv_path, columns)\n",
    "    completed = set(zip(existing_df.get('strategy', []), existing_df.get('n_per_class', [])))\n",
    "    \n",
    "    strategies = ['diverse', 'same_src', 'random']\n",
    "    n_values = [1, 2, 3]\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        for n in n_values:\n",
    "            if (strategy, n) in completed:\n",
    "                print(f\"  Skipping {strategy}/n={n} (already done)\")\n",
    "                continue\n",
    "            \n",
    "            examples = get_few_shot_examples(train_df, n_per_class=n, use_set=strategy)\n",
    "            results = evaluate_prompting(model, dev_df, mode=\"few-shot\", examples=examples, verbose=False)\n",
    "            \n",
    "            row = {\n",
    "                'strategy': strategy, \n",
    "                'n_per_class': n, \n",
    "                'macro_f1': results['macro_f1'],\n",
    "                **{f\"{k}_f1\": v for k, v in results['per_class_f1'].items()}\n",
    "            }\n",
    "            append_row_to_csv(csv_path, row)\n",
    "            print(f\"  {strategy}/n={n}: macro_f1={results['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class importance in few shot examples\n",
    "def run_exp3_class_importance(model, tokenizer, train_df, dev_df):    \n",
    "    csv_path = f\"{RESULTS_DIR}exp3_class_importance.csv\"\n",
    "    columns = ['combo_name', 'classes', 'support_f1', 'deny_f1', 'query_f1', 'comment_f1', 'macro_f1']\n",
    "    \n",
    "    existing_df = load_or_create_csv(csv_path, columns)\n",
    "    completed = set(existing_df.get('combo_name', []))\n",
    "    \n",
    "    all_classes = ['support', 'deny', 'query', 'comment']\n",
    "    class_abbrev = {'support': 'S', 'deny': 'D', 'query': 'Q', 'comment': 'C'}\n",
    "    \n",
    "    # generate all combinations: 1-shot singles, 2-shot pairs, 3-shot triples, 4-shot full\n",
    "    combos = []\n",
    "    for r in range(1, 5):\n",
    "        for combo in combinations(all_classes, r):\n",
    "            combos.append(list(combo))\n",
    "    \n",
    "    for classes in combos:\n",
    "        combo_name = f\"{len(classes)}-shot: \" + \"+\".join(class_abbrev[c] for c in classes)\n",
    "        \n",
    "        if combo_name in completed:\n",
    "            print(f\"  Skipping {combo_name} (already done)\")\n",
    "            continue\n",
    "        \n",
    "        examples = get_few_shot_examples(train_df, n_per_class=1, use_set='diverse', classes=classes)\n",
    "        results = evaluate_prompting(model, dev_df, mode=\"few-shot\", examples=examples, verbose=False)\n",
    "        \n",
    "        row = {\n",
    "            'combo_name': combo_name, \n",
    "            'classes': '+'.join(classes), \n",
    "            'macro_f1': results['macro_f1'],\n",
    "            **{f\"{k}_f1\": v for k, v in results['per_class_f1'].items()}\n",
    "        }\n",
    "        append_row_to_csv(csv_path, row)\n",
    "        print(f\"  {combo_name}: macro_f1={results['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison between all methods\n",
    "def run_exp4_test_final(model, tokenizer, train_df, test_df):\n",
    "    csv_path = f\"{RESULTS_DIR}exp4_test_final.csv\"\n",
    "    columns = ['strategy', 'repeat', 'macro_f1', 'support_f1', 'deny_f1', 'query_f1', 'comment_f1']\n",
    "    \n",
    "    existing_df = load_or_create_csv(csv_path, columns)\n",
    "    completed = set(zip(existing_df.get('strategy', []), existing_df.get('repeat', [])))\n",
    "    \n",
    "    strategies = ['zero-shot', 'few-shot', 'cot', 'few-shot-cot']\n",
    "    n_repeats = 3\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        for repeat in range(1, n_repeats + 1):\n",
    "            if (strategy, repeat) in completed:\n",
    "                print(f\"  Skipping {strategy}/repeat={repeat} (already done)\")\n",
    "                continue\n",
    "            \n",
    "            # Configure examples based on strategy\n",
    "            if strategy == 'zero-shot':\n",
    "                examples = None\n",
    "                mode = 'zero-shot'\n",
    "            elif strategy == 'few-shot':\n",
    "                examples = get_few_shot_examples(train_df, n_per_class=1, use_set='diverse')\n",
    "                mode = 'few-shot'\n",
    "            elif strategy == 'cot':\n",
    "                examples = None\n",
    "                mode = 'cot'\n",
    "            elif strategy == 'few-shot-cot':\n",
    "                examples = get_cot_examples(train_df)\n",
    "                mode = 'cot'\n",
    "            \n",
    "            results = evaluate_prompting(model, test_df, mode=mode, examples=examples, verbose=False)\n",
    "            \n",
    "            row = {\n",
    "                'strategy': strategy, \n",
    "                'repeat': repeat, \n",
    "                'macro_f1': results['macro_f1'],\n",
    "                **{f\"{k}_f1\": v for k, v in results['per_class_f1'].items()}\n",
    "            }\n",
    "            append_row_to_csv(csv_path, row)\n",
    "            print(f\"  {strategy}/repeat={repeat}: macro_f1={results['macro_f1']:.4f}\")\n",
    "    \n",
    "    print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get plots from experiment csvs if they exist\n",
    "# (seperate so i don't have to run experiment again to edit graph)\n",
    "def generate_all_plots():    \n",
    "    # system prompt ablation\n",
    "    csv_path = f\"{RESULTS_DIR}exp1_ablation.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        results_df = pd.read_csv(csv_path)\n",
    "        for config_type in ['isolated', 'cumulative']:\n",
    "            subset = results_df[results_df['config_type'] == config_type]\n",
    "            if len(subset) > 0:\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                sns.barplot(data=subset, y='config_name', x='macro_f1', hue='config_name', palette=custom_ablation_palette, ax=ax)\n",
    "                ax.set_xlabel('Macro F1')\n",
    "                ax.set_ylabel('')\n",
    "                ax.set_title(f'System Prompt Ablation: {config_type.title()}')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f\"{RESULTS_DIR}exp1_ablation_{config_type}.png\", dpi=150)\n",
    "                plt.close()\n",
    "    \n",
    "    # few-shot seletion strategies\n",
    "    csv_path = f\"{RESULTS_DIR}exp2_fewshot_strategies.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        results_df = pd.read_csv(csv_path)\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        sns.barplot(data=results_df, x='n_per_class', y='macro_f1', hue='strategy', ax=ax)\n",
    "        ax.set_xlabel('Examples per Stance')\n",
    "        ax.set_ylabel('Macro F1')\n",
    "        ax.set_title('Few-shot Strategy Comparison')\n",
    "        ax.legend(title='Strategy', loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}exp2_fewshot_strategies.png\", dpi=150)\n",
    "        plt.close()\n",
    "    \n",
    "    # few shot class importance heatmap\n",
    "    csv_path = f\"{RESULTS_DIR}exp3_class_importance.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        results_df = pd.read_csv(csv_path)\n",
    "        heatmap_data = results_df.set_index('combo_name')[['support_f1', 'deny_f1', 'query_f1', 'comment_f1', 'macro_f1']]\n",
    "        fig, ax = plt.subplots(figsize=(10, 12))\n",
    "        sns.heatmap(heatmap_data, annot=True, fmt='.2f', ax=ax, vmin=0.15, vmax=0.65)\n",
    "        ax.set_title('Class Importance in Few-shot Examples')\n",
    "        ax.set_xlabel('F1 Score')\n",
    "        ax.set_ylabel('Few-shot Combination')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{RESULTS_DIR}exp3_class_importance.png\", dpi=150)\n",
    "        plt.close()\n",
    "    \n",
    "    # comparison between all classification methods\n",
    "    csv_path = f\"{RESULTS_DIR}exp4_test_final.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        results_df = pd.read_csv(csv_path)\n",
    "        # results_df = results_df[results_df['strategy'] != 'classifier']\n",
    "        if len(results_df) > 0:\n",
    "            summary = results_df.groupby('strategy')['macro_f1'].agg(['mean', 'std']).reset_index()\n",
    "            summary.columns = ['strategy', 'mean_f1', 'std_f1']\n",
    "            order = ['classifier', 'zero-shot', 'few-shot', 'cot', 'few-shot-cot']\n",
    "            summary['strategy'] = pd.Categorical(summary['strategy'], categories=order, ordered=True)\n",
    "            summary = summary.sort_values('strategy')\n",
    "            \n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            bars = ax.bar(summary['strategy'], summary['mean_f1'], yerr=summary['std_f1'], capsize=5)\n",
    "            ax.set_xlabel('Strategy')\n",
    "            ax.set_ylabel('Macro F1 (mean ± std)')\n",
    "            ax.set_title('Test Set Performance Comparison (3 repeats)')\n",
    "            ax.set_ylim(0, 0.75)\n",
    "            for bar, mean in zip(bars, summary['mean_f1']):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, f'{mean:.3f}', \n",
    "                        ha='center', va='bottom', fontsize=10)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{RESULTS_DIR}exp4_test_final.png\", dpi=150)\n",
    "            plt.close()\n",
    "    \n",
    "    print(\"Plots saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gl/db7nsz8j70v_n0_38tnxspz80000gn/T/ipykernel_72266/3753050664.py:12: UserWarning: The palette list has more values (11) than needed (6), which may not be intended.\n",
      "  sns.barplot(data=subset, y='config_name', x='macro_f1', hue='config_name', palette=custom_ablation_palette, ax=ax)\n",
      "/var/folders/gl/db7nsz8j70v_n0_38tnxspz80000gn/T/ipykernel_72266/3753050664.py:12: UserWarning: The palette list has more values (11) than needed (5), which may not be intended.\n",
      "  sns.barplot(data=subset, y='config_name', x='macro_f1', hue='config_name', palette=custom_ablation_palette, ax=ax)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved.\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# run_exp1_ablation(model, tokenizer, train_df, dev_df)\n",
    "# run_exp2_fewshot_strategies(model, tokenizer, train_df, dev_df)\n",
    "# run_exp3_class_importance(model, tokenizer, train_df, dev_df)\n",
    "# run_exp4_test_final(model, tokenizer, train_df, test_df)\n",
    "\n",
    "generate_all_plots()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312k",
   "language": "python",
   "name": "venv312k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
