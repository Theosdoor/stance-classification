{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from transformers import pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import load_dataset, format_input_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from saved_data/datasets.pkl...\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "BATCH_SIZE = 16 # 12 works\n",
    "USE_CTX = False # whether to use [CTX] parts of threads (too slow for True)\n",
    "\n",
    "\n",
    "RESULTS_DIR = \"./results/prompting/\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# load data\n",
    "train_df, dev_df, test_df = load_dataset()\n",
    "\n",
    "VALID_STANCES = ['SUPPORT', 'DENY', 'QUERY', 'COMMENT']\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "PERSONA = \"\"\"You are an expert in rumour stance analysis on twitter.\"\"\"\n",
    "INSTRUCTION = \"\"\"Your task is to classify the stance of the TARGET tweet towards the veracity of the rumour in the SOURCE tweet.\"\"\"\n",
    "INPUT_FORMAT = \"\"\"\\\n",
    "    The input will be provided as a single string containing labeled segments:\n",
    "    \"[SRC] ... [PARENT] ... [TARGET] ...\" (Note: [PARENT] may be omitted if not applicable).\n",
    "    \"\"\"\n",
    "LABEL_DEFNS = \"\"\"\\\n",
    "    Classification Labels:\n",
    "    - SUPPORT: The reply supports the veracity of the source claim\n",
    "    - DENY: The reply denies the veracity of the source claim\n",
    "    - QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
    "    - COMMENT: The reply makes their own comment without a clear contribution to assessing the veracity of the source claim\n",
    "    \"\"\"\n",
    "OUTPUT_FORMAT = \"\"\"\\\n",
    "    Respond with ONLY one word: SUPPORT, DENY, QUERY, or COMMENT.\n",
    "    \"\"\"\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "{OUTPUT_FORMAT}\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Text: {thread_context}\n",
    "\n",
    "Task: Classify the stance of [TARGET] towards the veracity of the rumour in [SRC].\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(thread_context):\n",
    "    return USER_PROMPT_TEMPLATE.format(thread_context=thread_context)\n",
    "\n",
    "\n",
    "def build_zero_shot_messages(thread_context):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": build_user_prompt(thread_context)},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_few_shot_messages(thread_context, examples=None):\n",
    "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}]\n",
    "    \n",
    "    if examples:\n",
    "        for ex in examples:\n",
    "            messages.append({\"role\": \"user\", \"content\": build_user_prompt(ex['source'])})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ex['label']})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": build_user_prompt(thread_context)})\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stance_response(text):\n",
    "    text = text.upper().strip()\n",
    "    for label in VALID_STANCES:\n",
    "        if re.search(rf'\\b{label}\\b', text):\n",
    "            return label.lower()\n",
    "    print(f'Error! On {text}\\n')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(pipe, messages, max_new_tokens=10): # max n tokens to limit response length\n",
    "    output = pipe(\n",
    "        messages, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False, # deterministic output\n",
    "        pad_token_id=pipe.tokenizer.eos_token_id, # prevent warnings\n",
    "        )\n",
    "    return output[0][\"generated_text\"][-1][\"content\"].strip()\n",
    "\n",
    "\n",
    "# LLM --\n",
    "def plot_confusion_matrix(cm, mode, save_path=None):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix ({mode})')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_results(results, model_key, mode):\n",
    "    \"\"\"Save evaluation results to JSON.\"\"\"\n",
    "    output = {\n",
    "        'model': model_key,\n",
    "        'mode': mode,\n",
    "        'macro_f1': results['macro_f1'],\n",
    "        'per_class_f1': results['per_class_f1'],\n",
    "        'predictions': results['predictions'],\n",
    "        'true_labels': results['true_labels'],\n",
    "        'raw_responses': results['raw_responses']\n",
    "    }\n",
    "    \n",
    "    filename = f\"{RESULTS_DIR}{model_key}_{mode}_results.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot examples\n",
    "support_example = train_df[train_df['label_text']=='support'].iloc[0]\n",
    "deny_example = train_df[train_df['label_text']=='deny'].iloc[0]\n",
    "query_example = train_df[train_df['label_text']=='query'].iloc[0]\n",
    "comment_example = train_df[train_df['label_text']=='comment'].iloc[0]\n",
    "\n",
    "\n",
    "def get_few_shot_examples(df, n_per_class=1):\n",
    "    \"\"\"Select stratified random examples for few-shot prompting.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for label in ['support', 'deny', 'query', 'comment']:\n",
    "        class_df = df[df['label_text'] == label]\n",
    "        samples = class_df.sample(n=min(n_per_class, len(class_df)))\n",
    "        \n",
    "        for _, row in samples.iterrows():\n",
    "            examples.append({\n",
    "                'source': format_input_with_context(row, df, use_features=False, use_context=USE_CTX),\n",
    "                'label': label.upper()\n",
    "            })\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa843fbf2f047b2ac3ca86c8eaafb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    MODEL_NAME, \n",
    "    device_map='auto',\n",
    "    dtype=torch.float16,\n",
    ")\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "pipe.tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(MODEL_NAME)\n",
    "print(BATCH_SIZE)\n",
    "print(pipe.model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompting(model, df, mode=\"zero-shot\", examples=None, batch_size=BATCH_SIZE, verbose=True):\n",
    "    from datasets import Dataset\n",
    "    \n",
    "    # Prepare all messages upfront\n",
    "    all_messages = []\n",
    "    for _, row in df.iterrows():\n",
    "        input_text = format_input_with_context(row, df, use_features=False, use_context=USE_CTX)\n",
    "        if mode == \"zero-shot\":\n",
    "            messages = build_zero_shot_messages(input_text)\n",
    "        else:\n",
    "            messages = build_few_shot_messages(input_text, examples)\n",
    "        all_messages.append(messages)\n",
    "    \n",
    "    # Batched inference using Dataset\n",
    "    dataset = Dataset.from_dict({\"messages\": all_messages})\n",
    "    \n",
    "    raw_responses = []\n",
    "    for i in tqdm(range(0, len(all_messages), batch_size), desc=f\"Evaluating ({mode})\"):\n",
    "        batch = all_messages[i:i+batch_size]\n",
    "        outputs = pipe(batch, max_new_tokens=10, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "        for out in outputs:\n",
    "            raw_responses.append(out[0][\"generated_text\"][-1][\"content\"].strip())\n",
    "    \n",
    "    # Parse responses\n",
    "    predictions = [parse_stance_response(r) for r in raw_responses]\n",
    "\n",
    "    # count number of None (errors), print and replace with comment\n",
    "    num_errors = predictions.count(None)\n",
    "    if num_errors > 0:\n",
    "        print(f\"Warning: {num_errors} errors in predictions. Replacing with 'comment'.\")\n",
    "        predictions = [p if p is not None else 'comment' for p in predictions]\n",
    "\n",
    "    true_labels = df['label_text'].tolist()\n",
    "    \n",
    "    # Metrics\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    per_class_f1 = f1_score(true_labels, predictions, average=None, labels=labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\\nResults ({mode})\\n{'='*60}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"\\nPer-class F1:\")\n",
    "        for lbl, f1 in zip(labels, per_class_f1):\n",
    "            print(f\"  {lbl}: {f1:.4f}\")\n",
    "        print(f\"\\n{classification_report(true_labels, predictions, labels=labels)}\")\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'true_label': true_labels,\n",
    "        'predicted': predictions,\n",
    "    })\n",
    "    csv_path = f\"{RESULTS_DIR}predictions_{mode}.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    if verbose:\n",
    "        print(f\"Saved predictions: {csv_path}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'raw_responses': raw_responses,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': dict(zip(labels, per_class_f1)),\n",
    "        'confusion_matrix': confusion_matrix(true_labels, predictions, labels=labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8009667f1e804dd492a4d70ad5f7d08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (zero-shot):   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results (zero-shot)\n",
      "============================================================\n",
      "Macro F1: 0.2110\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.0000\n",
      "  deny: 0.1739\n",
      "  query: 0.2292\n",
      "  comment: 0.4409\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.00      0.00      0.00        69\n",
      "        deny       0.11      0.36      0.17        11\n",
      "       query       0.13      0.79      0.23        28\n",
      "     comment       0.69      0.32      0.44       173\n",
      "\n",
      "    accuracy                           0.29       281\n",
      "   macro avg       0.23      0.37      0.21       281\n",
      "weighted avg       0.44      0.29      0.30       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_zero-shot.csv\n"
     ]
    }
   ],
   "source": [
    "# zero-shot\n",
    "\n",
    "zero_results = evaluate_prompting(pipe, dev_df, mode=\"zero-shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: comment\n",
      "Response:     QUERY\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb22d29242443f281b99d421b2397a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (few-shot):   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error! On RE-POST\n",
      "\n",
      "Error! On REPOST\n",
      "\n",
      "Error! On REPEAT\n",
      "\n",
      "Error! On REPEAT\n",
      "\n",
      "Warning: 4 errors in predictions. Replacing with 'comment'.\n",
      "\n",
      "============================================================\n",
      "Results (few-shot)\n",
      "============================================================\n",
      "Macro F1: 0.3654\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.3306\n",
      "  deny: 0.2632\n",
      "  query: 0.3623\n",
      "  comment: 0.5057\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.38      0.29      0.33        69\n",
      "        deny       0.19      0.45      0.26        11\n",
      "       query       0.23      0.89      0.36        28\n",
      "     comment       0.73      0.39      0.51       173\n",
      "\n",
      "    accuracy                           0.42       281\n",
      "   macro avg       0.38      0.51      0.37       281\n",
      "weighted avg       0.57      0.42      0.44       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_few-shot.csv\n"
     ]
    }
   ],
   "source": [
    "# few-shot\n",
    "few_shot_examples = get_few_shot_examples(train_df, n_per_class=1)\n",
    "few_results = evaluate_prompting(pipe, df, mode=\"few-shot\", examples=few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312k",
   "language": "python",
   "name": "venv312k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
