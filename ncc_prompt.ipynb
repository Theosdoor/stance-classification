{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "# from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import outlines # see https://dottxt-ai.github.io/outlines/latest/features/models/transformers/\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import load_dataset, format_input_with_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached data from saved_data/datasets.pkl...\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "BATCH_SIZE = 12\n",
    "USE_CTX = False # whether to use [CTX] parts of threads (too slow for True)\n",
    "MAX_COT_TOKENS = 100 # max n tokens for cot response\n",
    "\n",
    "RESULTS_DIR = \"./results/prompting/\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# load data\n",
    "train_df, dev_df, test_df = load_dataset()\n",
    "use_df = dev_df # df to test on\n",
    "\n",
    "VALID_STANCES = ['SUPPORT', 'DENY', 'QUERY', 'COMMENT']\n",
    "\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-3B-Instruct\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "\n",
    "RELOAD_MODEL = False\n",
    "\n",
    "# output types for outlines\n",
    "from typing import Literal\n",
    "StanceLabel = Literal[\"SUPPORT\", \"DENY\", \"QUERY\", \"COMMENT\"]\n",
    "COT_REGEX = outlines.types.Regex(r\".*Label: (SUPPORT|DENY|QUERY|COMMENT)\")\n",
    "\n",
    "if 'model' not in globals() or RELOAD_MODEL:\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.float16,\n",
    "        device_map='auto',\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # use outlines\n",
    "    model = outlines.from_transformers(hf_model, tokenizer)\n",
    "    \n",
    "    # get rid of annoying warnings re pad_token\n",
    "    model.tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.tokenizer.padding_side = \"left\"\n",
    "    model.model.generation_config.pad_token_id = model.tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "# if 'pipe' not in globals() or RELOAD_MODEL: # rerunning takes up more mem\n",
    "#     pipe = pipeline(\n",
    "#         \"text-generation\", \n",
    "#         MODEL_NAME, \n",
    "#         device_map='auto',\n",
    "#         dtype=torch.float16,\n",
    "#     )\n",
    "#     pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n",
    "#     pipe.tokenizer.padding_side = \"left\"\n",
    "\n",
    "# print(pipe.model.config)\n",
    "\n",
    "print(MODEL_NAME)\n",
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts\n",
    "# see https://huggingface.co/docs/transformers/en/tasks/prompting\n",
    "\n",
    "PERSONA = \"\"\"You are an expert in rumour stance analysis on twitter.\"\"\"\n",
    "INSTRUCTION = \"\"\"Your task is to classify the stance of the [TARGET] tweet towards the veracity of the rumour in the [SRC] tweet.\"\"\"\n",
    "INPUT_FORMAT = \"\"\"\\\n",
    "    The input will be provided as a single string containing labeled segments:\n",
    "    \"[SRC] ... [PARENT] ... [TARGET] ...\". (Note: [PARENT] is omitted if [TARGET] replies directly to [SRC])\n",
    "    \"\"\"\n",
    "# LABEL_DEFNS = \"\"\"\\\n",
    "#     Classification Labels:\n",
    "#     - SUPPORT: The reply supports the veracity of the source claim\n",
    "#     - DENY: The reply denies the veracity of the source claim\n",
    "#     - QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
    "#     - COMMENT: The reply makes their own comment without a clear contribution to assessing the veracity of the source claim\n",
    "#     \"\"\"\n",
    "LABEL_DEFNS = \"\"\"\\\n",
    "    Classification Labels:\n",
    "    - SUPPORT: The reply supports veracity of the source claim\n",
    "    - DENY: The reply denies the veracity of the source claim\n",
    "    - QUERY: The reply asks for additional evidence in relation to the veracity of the source claim\n",
    "    - COMMENT: The reply makes their own comment without a clear contribution to assessing the veracity of the source claim\n",
    "    \"\"\"\n",
    "\n",
    "OUTPUT_FORMAT = \"\"\"\\\n",
    "    Respond with ONLY one word: SUPPORT, DENY, QUERY, or COMMENT.\n",
    "    \"\"\"\n",
    "\n",
    "SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "{OUTPUT_FORMAT}\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Text: {thread_context}\n",
    "\n",
    "Task: Classify the stance of [TARGET] towards the veracity of the rumour in [SRC].\n",
    "\"\"\"\n",
    "\n",
    "def build_user_prompt(thread_context):\n",
    "    return USER_PROMPT_TEMPLATE.format(thread_context=thread_context)\n",
    "\n",
    "\n",
    "def build_zero_shot_messages(thread_context):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": build_user_prompt(thread_context)},\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_few_shot_messages(thread_context, examples=None):\n",
    "    messages = [{\"role\": \"system\", \"content\": SYS_PROMPT}]\n",
    "    \n",
    "    if examples:\n",
    "        for ex in examples:\n",
    "            messages.append({\"role\": \"user\", \"content\": build_user_prompt(ex['source'])})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ex['label']})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": build_user_prompt(thread_context)})\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COT prompts \n",
    "\n",
    "# handselected ids for reasoning\n",
    "COT_FEW_SHOT_IDS = {\n",
    "    'support': '524967134339022848', # @TheKirkness Radio Canada tweeting same. must be true :-(\n",
    "    'deny': '544292581950357504', # @JSchoenberger7 @TheAnonMessage not an Isis flag. Just an Islamic one. Stop spreading false rumors.\n",
    "    'query': '544281192632053761', # @mscott Are your reporters 100% sure it is an ISIS flag? Cause that is what is being reported. #facts\n",
    "    'comment': '552804023389392896', # @thei100 @Independent these fuckers thinking its a GTA heist mission\n",
    "}\n",
    "\n",
    "COT_EXAMPLES = {\n",
    "    'support': \"1. Stance: takes position on veracity\\n2. \\\"must be true\\\" → affirms claim\\nLabel: SUPPORT\",\n",
    "    'deny': \"1. Stance: challenges veracity\\n2. \\\"not an Isis flag\\\", \\\"false rumors\\\" → rejects claim\\nLabel: DENY\",\n",
    "    'query': \"1. Stance: engages with veracity\\n2. \\\"Are your reporters 100% sure?\\\" → requests evidence\\nLabel: QUERY\",\n",
    "    'comment': \"1. Stance: no veracity assessment\\n2. Offers opinion/reaction only\\nLabel: COMMENT\",\n",
    "}\n",
    "\n",
    "\n",
    "COT_OUTPUT_FORMAT = \"\"\"\\\n",
    "Think BRIEFLY step-by-step (max 2-3 short lines).\n",
    "End with \"Label: \" followed by ONLY one word: SUPPORT, DENY, QUERY, or COMMENT.\n",
    "\"\"\"\n",
    "\n",
    "COT_SYS_PROMPT = f\"\"\"\\\n",
    "{PERSONA}\n",
    "{INSTRUCTION}\n",
    "{INPUT_FORMAT}\n",
    "{LABEL_DEFNS}\n",
    "{COT_OUTPUT_FORMAT}\n",
    "\"\"\"\n",
    "\n",
    "def build_cot_messages(thread_context, examples=None):\n",
    "    messages = [{\"role\": \"system\", \"content\": COT_SYS_PROMPT}]\n",
    "    \n",
    "    if examples:\n",
    "        for ex in examples:\n",
    "            messages.append({\"role\": \"user\", \"content\": build_user_prompt(ex['source'])})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": ex['reasoning']})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": build_user_prompt(thread_context)})\n",
    "    return messages\n",
    "\n",
    "USER_COT_PROMPT_TEMPLATE = \"\"\"\\\n",
    "Text: {thread_context}\n",
    "\n",
    "Let's go through this step-by-step:\n",
    "\"\"\"\n",
    "\n",
    "def build_cot_user_prompt(thread_context):\n",
    "    return USER_COT_PROMPT_TEMPLATE.format(thread_context=thread_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_stance_response(text):\n",
    "#     text = text.upper().strip()\n",
    "#     for label in VALID_STANCES:\n",
    "#         if re.search(rf'\\b{label}\\b', text):\n",
    "#             return label.lower()\n",
    "#     return None\n",
    "\n",
    "def parse_cot_label(text):\n",
    "    \"\"\"Extract the stance label from CoT output (after 'Label: ').\"\"\"\n",
    "    match = re.search(r'Label:\\s*(SUPPORT|DENY|QUERY|COMMENT)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).lower()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_response(pipe, messages): \n",
    "#     output = pipe(\n",
    "#         messages, \n",
    "#         max_new_tokens=10,\n",
    "#         do_sample=False, # deterministic output\n",
    "#         pad_token_id=pipe.tokenizer.eos_token_id, # prevent warnings\n",
    "#         )\n",
    "#     return output[0][\"generated_text\"][-1][\"content\"].strip()\n",
    "\n",
    "\n",
    "\n",
    "# LLM --\n",
    "def plot_confusion_matrix(cm, mode, save_path=None):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix ({mode})')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_results(results, model_key, mode):\n",
    "    \"\"\"Save evaluation results to JSON.\"\"\"\n",
    "    output = {\n",
    "        'model': model_key,\n",
    "        'mode': mode,\n",
    "        'macro_f1': results['macro_f1'],\n",
    "        'per_class_f1': results['per_class_f1'],\n",
    "        'predictions': results['predictions'],\n",
    "        'true_labels': results['true_labels'],\n",
    "        'raw_responses': results['raw_responses']\n",
    "    }\n",
    "    \n",
    "    filename = f\"{RESULTS_DIR}{model_key}_{mode}_results.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shot examples\n",
    "support_example = train_df[train_df['label_text']=='support'].iloc[0]\n",
    "deny_example = train_df[train_df['label_text']=='deny'].iloc[0]\n",
    "query_example = train_df[train_df['label_text']=='query'].iloc[0]\n",
    "comment_example = train_df[train_df['label_text']=='comment'].iloc[0]\n",
    "\n",
    "\n",
    "def get_few_shot_examples(df, n_per_class=1):\n",
    "    \"\"\"Select stratified random examples for few-shot prompting.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    for label in ['support', 'deny', 'query', 'comment']:\n",
    "        class_df = df[df['label_text'] == label]\n",
    "        samples = class_df.sample(n=min(n_per_class, len(class_df)))\n",
    "        \n",
    "        for _, row in samples.iterrows():\n",
    "            examples.append({\n",
    "                'source': format_input_with_context(row, df, use_features=False, use_context=USE_CTX),\n",
    "                'label': label.upper()\n",
    "            })\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompting(model, df, mode=\"zero-shot\", examples=None, batch_size=BATCH_SIZE, verbose=True):\n",
    "#     from datasets import Dataset\n",
    "    \n",
    "#     if mode == \"cot\":\n",
    "#         max_new_tokens = MAX_COT_TOKENS\n",
    "#     else:\n",
    "#         max_new_tokens = 10\n",
    "    \n",
    "#     # Prepare all messages upfront\n",
    "#     all_messages = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         input_text = format_input_with_context(row, df, use_features=False, use_context=USE_CTX)\n",
    "#         if mode == \"zero-shot\":\n",
    "#             messages = build_zero_shot_messages(input_text)\n",
    "#         elif mode == \"few-shot\":\n",
    "#             messages = build_few_shot_messages(input_text, examples)\n",
    "#         elif mode == \"cot\":\n",
    "#             messages = build_cot_messages(input_text, examples)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unrecognised mode: {mode}. Choose from 'zero-shot', 'few-shot', or 'cot'.\")\n",
    "#         all_messages.append(messages)\n",
    "    \n",
    "#     # Batched inference using Dataset\n",
    "#     dataset = Dataset.from_dict({\"messages\": all_messages})\n",
    "    \n",
    "#     raw_responses = []\n",
    "#     for i in tqdm(range(0, len(all_messages), batch_size), desc=f\"Evaluating ({mode})\"):\n",
    "#         batch = all_messages[i:i+batch_size]\n",
    "#         outputs = pipe(batch, max_new_tokens=max_new_tokens, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "#         for out in outputs:\n",
    "#             raw_responses.append(out[0][\"generated_text\"][-1][\"content\"].strip())\n",
    "    \n",
    "#     # Parse responses\n",
    "#     predictions = [parse_stance_response(r) for r in raw_responses]\n",
    "\n",
    "#     # count number of None (errors), print and replace with comment\n",
    "#     error_idxs = [i for i, p in enumerate(predictions) if p is None]\n",
    "#     num_errors = len(error_idxs)  \n",
    "    \n",
    "#     if error_idxs:\n",
    "#         print(f\"Warning: {num_errors} errors in predictions. Replacing with 'comment'.\")\n",
    "#         for idx in error_idxs:\n",
    "#             problem_row = df.iloc[idx]['text']\n",
    "#             raw_text = raw_responses[idx]\n",
    "            \n",
    "#             print(f\"Index: {idx}\")\n",
    "#             print(f\"Raw Model Output: '{raw_text}'\")\n",
    "#             # Adjust 'text' to whatever column contains your input content\n",
    "#             print(f\"Input Data: {problem_row}\") \n",
    "#             print(\"-\" * 30)\n",
    "    \n",
    "#     if num_errors > 0:\n",
    "#         predictions = [p if p is not None else 'comment' for p in predictions]\n",
    "\n",
    "\n",
    "    if mode == \"cot\":\n",
    "        output_type = COT_REGEX\n",
    "        max_new_tokens = MAX_COT_TOKENS\n",
    "    else:\n",
    "        output_type = StanceLabel\n",
    "        max_new_tokens = 10\n",
    "    \n",
    "    # Prepare all prompts\n",
    "    all_prompts = []\n",
    "    for _, row in df.iterrows():\n",
    "        input_text = format_input_with_context(row, df, use_features=False, use_context=USE_CTX)\n",
    "        if mode == \"zero-shot\":\n",
    "            messages = build_zero_shot_messages(input_text)\n",
    "        elif mode == \"few-shot\":\n",
    "            messages = build_few_shot_messages(input_text, examples)\n",
    "        elif mode == \"cot\":\n",
    "            messages = build_cot_messages(input_text, examples)\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognised mode: {mode}. Choose from 'zero-shot', 'few-shot', or 'cot'.\")\n",
    "        \n",
    "        # Convert messages to prompt string\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        all_prompts.append(prompt)\n",
    "    \n",
    "    # Generate with constraints (one at a time - outlines doesn't batch well with constraints)\n",
    "    raw_responses = []\n",
    "    predictions = []\n",
    "    \n",
    "    for prompt in tqdm(all_prompts, desc=f\"Evaluating ({mode})\"):\n",
    "        response = model(prompt, output_type, max_new_tokens=max_new_tokens)\n",
    "        raw_responses.append(response)\n",
    "        \n",
    "        # Parse based on mode\n",
    "        if mode == \"cot\":\n",
    "            # Extract label from CoT response\n",
    "            pred = parse_cot_label(response)\n",
    "        else:\n",
    "            # Direct label output (already constrained)\n",
    "            pred = response.lower() if response in VALID_STANCES else None\n",
    "        \n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Handle any parsing errors (should be rare with constraints)\n",
    "    error_idxs = [i for i, p in enumerate(predictions) if p is None]\n",
    "    num_errors = len(error_idxs)\n",
    "    \n",
    "    if error_idxs and verbose:\n",
    "        print(f\"Warning: {num_errors} errors in predictions. Replacing with 'comment'.\")\n",
    "        for idx in error_idxs[:5]:  # Show first 5 errors only\n",
    "            print(f\"Index: {idx}\")\n",
    "            print(f\"Raw Model Output: '{raw_responses[idx]}'\")\n",
    "            print(\"-\" * 30)\n",
    "    \n",
    "    predictions = [p if p is not None else 'comment' for p in predictions]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    true_labels = df['label_text'].tolist()\n",
    "    \n",
    "    # Metrics\n",
    "    labels = ['support', 'deny', 'query', 'comment']\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    per_class_f1 = f1_score(true_labels, predictions, average=None, labels=labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\\nResults ({mode})\\n{'='*60}\")\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "        print(f\"\\nPer-class F1:\")\n",
    "        for lbl, f1 in zip(labels, per_class_f1):\n",
    "            print(f\"  {lbl}: {f1:.4f}\")\n",
    "        print(f\"\\n{classification_report(true_labels, predictions, labels=labels, zero_division=0.0)}\")\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'true_label': true_labels,\n",
    "        'predicted': predictions,\n",
    "    })\n",
    "    csv_path = f\"{RESULTS_DIR}predictions_{mode}.csv\"\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    if verbose:\n",
    "        print(f\"Saved predictions: {csv_path}\")\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'raw_responses': raw_responses,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': dict(zip(labels, per_class_f1)),\n",
    "        'confusion_matrix': confusion_matrix(true_labels, predictions, labels=labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4e357820144e5283d4e53e6cab0ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (zero-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] WON'T CONVERT _apply_token_bitmask_inplace_kernel /home2/nchw73/venv312/lib/python3.12/site-packages/outlines_core/kernels/torch.py line 43 \n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] due to: \n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] Traceback (most recent call last):\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1433, in _compile\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_utils_internal.py\", line 92, in wrapper_function\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return function(*args, **kwargs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1117, in compile_inner\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return _compile_inner(code, one_graph, hooks)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1151, in _compile_inner\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     dynamo_output = compile_frame(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                     ^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1032, in compile_frame\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     bytecode, tracer_output = transform_code_object(code, transform)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1592, in transform_code_object\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     tracer_output = transformations(instructions, code_options)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1004, in transform\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     tracer_output = trace_frame(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                     ^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 312, in _fn\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return fn(*args, **kwargs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 815, in trace_frame\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     run_tracer()\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 797, in run_tracer\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     tracer.run()\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 1487, in run\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     while self.step():\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]           ^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 1348, in step\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 4089, in RETURN_CONST\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     self._return(inst)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 4064, in _return\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     all_stack_locals_metadata = self.output.compile_subgraph(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1568, in compile_subgraph\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 2013, in compile_and_call_fx_graph\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     compiled_fn = self.call_user_compiler(gm, self.example_inputs())\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 2136, in call_user_compiler\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return self._call_user_compiler(gm, example_inputs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 2171, in _call_user_compiler\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 156, in __call__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/__init__.py\", line 2392, in __call__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 2406, in compile_fx\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     torch._inductor.async_compile.AsyncCompile.wakeup()\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/async_compile.py\", line 354, in wakeup\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     if not cls.use_process_pool():\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/async_compile.py\", line 331, in use_process_pool\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     cls._ready_future = cls.process_pool().submit(cls._get_ready)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                         ^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/async_compile.py\", line 278, in process_pool\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     pool = SubprocPool(get_compile_threads())\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 142, in __init__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     torch_key_str = base64.b64encode(torch_key()).decode(\"utf-8\")\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                                      ^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 685, in wrapper\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     _cache.append(func())\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 727, in torch_key\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return get_code_hash(_TORCH_PATH)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 720, in get_code_hash\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     build_code_hash([root], \"\", hasher)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 672, in build_code_hash\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     build_code_hash(spec.submodule_search_locations, f\"{spec.name}.\", hasher)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 672, in build_code_hash\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     build_code_hash(spec.submodule_search_locations, f\"{spec.name}.\", hasher)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 669, in build_code_hash\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     hasher.update(f.read())\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] KeyboardInterrupt\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] \n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] During handling of the above exception, another exception occurred:\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] \n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] Traceback (most recent call last):\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1624, in __call__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     result = self._inner_convert(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 688, in __call__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     result = _compile(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]              ^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1509, in _compile\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     if tracer_output:\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]        ^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] UnboundLocalError: cannot access local variable 'tracer_output' where it is not associated with a value\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] Traceback (most recent call last):\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1433, in _compile\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_utils_internal.py\", line 92, in wrapper_function\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return function(*args, **kwargs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1117, in compile_inner\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return _compile_inner(code, one_graph, hooks)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1151, in _compile_inner\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     dynamo_output = compile_frame(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                     ^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1032, in compile_frame\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     bytecode, tracer_output = transform_code_object(code, transform)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1592, in transform_code_object\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     tracer_output = transformations(instructions, code_options)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1004, in transform\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     tracer_output = trace_frame(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                     ^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 312, in _fn\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return fn(*args, **kwargs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 815, in trace_frame\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     run_tracer()\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 797, in run_tracer\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     tracer.run()\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 1487, in run\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     while self.step():\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]           ^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 1348, in step\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 4089, in RETURN_CONST\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     self._return(inst)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/symbolic_convert.py\", line 4064, in _return\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     all_stack_locals_metadata = self.output.compile_subgraph(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 1568, in compile_subgraph\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 2013, in compile_and_call_fx_graph\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     compiled_fn = self.call_user_compiler(gm, self.example_inputs())\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 2136, in call_user_compiler\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return self._call_user_compiler(gm, example_inputs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/output_graph.py\", line 2171, in _call_user_compiler\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/repro/after_dynamo.py\", line 156, in __call__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/__init__.py\", line 2392, in __call__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/compile_fx.py\", line 2406, in compile_fx\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     torch._inductor.async_compile.AsyncCompile.wakeup()\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/async_compile.py\", line 354, in wakeup\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     if not cls.use_process_pool():\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/async_compile.py\", line 331, in use_process_pool\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     cls._ready_future = cls.process_pool().submit(cls._get_ready)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                         ^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/async_compile.py\", line 278, in process_pool\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     pool = SubprocPool(get_compile_threads())\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 142, in __init__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     torch_key_str = base64.b64encode(torch_key()).decode(\"utf-8\")\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                                      ^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 685, in wrapper\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     _cache.append(func())\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 727, in torch_key\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     return get_code_hash(_TORCH_PATH)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 720, in get_code_hash\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     build_code_hash([root], \"\", hasher)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 672, in build_code_hash\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     build_code_hash(spec.submodule_search_locations, f\"{spec.name}.\", hasher)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 672, in build_code_hash\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     build_code_hash(spec.submodule_search_locations, f\"{spec.name}.\", hasher)\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_inductor/codecache.py\", line 669, in build_code_hash\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     hasher.update(f.read())\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]                   ^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] KeyboardInterrupt\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] \n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] During handling of the above exception, another exception occurred:\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] \n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] Traceback (most recent call last):\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1624, in __call__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     result = self._inner_convert(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 688, in __call__\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     result = _compile(\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]              ^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]   File \"/home2/nchw73/venv312/lib/python3.12/site-packages/torch/_dynamo/convert_frame.py\", line 1509, in _compile\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]     if tracer_output:\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707]        ^^^^^^^^^^^^^\n",
      "W0102 18:13:08.965000 373702 torch/_dynamo/convert_frame.py:1707] UnboundLocalError: cannot access local variable 'tracer_output' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results (zero-shot)\n",
      "============================================================\n",
      "Macro F1: 0.2300\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.0286\n",
      "  deny: 0.2069\n",
      "  query: 0.2110\n",
      "  comment: 0.4735\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       1.00      0.01      0.03        69\n",
      "        deny       0.17      0.27      0.21        11\n",
      "       query       0.12      0.82      0.21        28\n",
      "     comment       0.81      0.34      0.47       173\n",
      "\n",
      "    accuracy                           0.30       281\n",
      "   macro avg       0.52      0.36      0.23       281\n",
      "weighted avg       0.76      0.30      0.33       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_zero-shot.csv\n"
     ]
    }
   ],
   "source": [
    "# zero-shot\n",
    "zero_results = evaluate_prompting(model, use_df, mode=\"zero-shot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "\n",
    "# Component dictionary for easy reference\n",
    "PROMPT_COMPONENTS = {\n",
    "    'persona': PERSONA,\n",
    "    'instruction': INSTRUCTION,\n",
    "    'input_format': INPUT_FORMAT,\n",
    "    'label_defns': LABEL_DEFNS,\n",
    "    'output_format': OUTPUT_FORMAT,\n",
    "}\n",
    "\n",
    "# Build colour palette: light grey for minimal, seaborn default for components, black for full\n",
    "_default_palette = sns.color_palette()  # Default seaborn qualitative palette\n",
    "COMPONENT_COLOURS = {\n",
    "    'minimal': '#D3D3D3',              # Light grey\n",
    "    'persona': _default_palette[0],     # Blue\n",
    "    'instruction': _default_palette[1], # Orange\n",
    "    'input_format': _default_palette[2],# Green\n",
    "    'label_defns': _default_palette[3], # Red\n",
    "    'output_format': _default_palette[4],# Purple\n",
    "    'full': '#000000',                  # Black\n",
    "}\n",
    "\n",
    "# --- Ablation Configurations ---\n",
    "\n",
    "# ISOLATED: minimal + ONE component (shows individual contribution)\n",
    "ISOLATED_CONFIGS = {\n",
    "    'minimal': [],\n",
    "    '+persona': ['persona'],\n",
    "    '+instruction': ['instruction'],\n",
    "    '+input_format': ['input_format'],\n",
    "    '+label_defns': ['label_defns'],\n",
    "    '+output_format': ['output_format'],\n",
    "    'full': ['persona', 'instruction', 'input_format', 'label_defns', 'output_format'],\n",
    "}\n",
    "\n",
    "# CUMULATIVE: stacking components one by one\n",
    "CUMULATIVE_CONFIGS = {\n",
    "    'minimal': [],\n",
    "    '+persona': ['persona'],\n",
    "    '+instruction': ['persona', 'instruction'],\n",
    "    '+input_format': ['persona', 'instruction', 'input_format'],\n",
    "    '+label_defns': ['persona', 'instruction', 'input_format', 'label_defns'],\n",
    "    '+output_format (full)': ['persona', 'instruction', 'input_format', 'label_defns', 'output_format'],\n",
    "}\n",
    "\n",
    "\n",
    "def build_custom_sys_prompt(component_keys):\n",
    "    \"\"\"Build system prompt from list of component keys.\"\"\"\n",
    "    if not component_keys:\n",
    "        return None  # No system prompt\n",
    "    components = [PROMPT_COMPONENTS[k] for k in component_keys]\n",
    "    return \"\\n\".join(components)\n",
    "\n",
    "\n",
    "def build_zero_shot_messages_custom(thread_context, sys_prompt=None):\n",
    "    \"\"\"Build zero-shot messages with optional custom system prompt.\"\"\"\n",
    "    messages = []\n",
    "    if sys_prompt is not None:\n",
    "        messages.append({\"role\": \"system\", \"content\": sys_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": build_user_prompt(thread_context)})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def evaluate_with_custom_prompt(pipe, df, sys_prompt=None, batch_size=BATCH_SIZE, verbose=False):\n",
    "    \"\"\"Evaluate zero-shot with a custom system prompt. Returns macro F1.\"\"\"\n",
    "    # Prepare all messages\n",
    "    all_messages = []\n",
    "    for _, row in df.iterrows():\n",
    "        input_text = format_input_with_context(row, df, use_features=False, use_context=USE_CTX)\n",
    "        messages = build_zero_shot_messages_custom(input_text, sys_prompt)\n",
    "        all_messages.append(messages)\n",
    "    \n",
    "    # Batched inference\n",
    "    raw_responses = []\n",
    "    for i in range(0, len(all_messages), batch_size):\n",
    "        batch = all_messages[i:i+batch_size]\n",
    "        outputs = pipe(batch, max_new_tokens=10, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "        for out in outputs:\n",
    "            raw_responses.append(out[0][\"generated_text\"][-1][\"content\"].strip())\n",
    "    \n",
    "    # Parse responses\n",
    "    predictions = [parse_stance_response(r) for r in raw_responses]\n",
    "    predictions = [p if p is not None else 'comment' for p in predictions]\n",
    "    \n",
    "    true_labels = df['label_text'].tolist()\n",
    "    macro_f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Macro F1: {macro_f1:.4f}\")\n",
    "    \n",
    "    return macro_f1\n",
    "\n",
    "\n",
    "def run_ablation_study(pipe, df, configs, desc=\"Ablation\"):\n",
    "    \"\"\"Run ablation study with given configurations. Returns dict of {config_name: macro_f1}.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for config_name, component_keys in tqdm(configs.items(), desc=desc):\n",
    "        sys_prompt = build_custom_sys_prompt(component_keys)\n",
    "        macro_f1 = evaluate_with_custom_prompt(pipe, df, sys_prompt)\n",
    "        results[config_name] = macro_f1\n",
    "        print(f\"  {config_name}: {macro_f1:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _get_ablation_color(config_name):\n",
    "    \"\"\"Get bar color based on config name using seaborn palette.\"\"\"\n",
    "    if config_name == 'minimal':\n",
    "        return COMPONENT_COLOURS['minimal']\n",
    "    elif 'full' in config_name:\n",
    "        return COMPONENT_COLOURS['full']\n",
    "    # Extract component name (e.g., '+persona' -> 'persona')\n",
    "    for comp in COMPONENT_COLOURS:\n",
    "        if comp in config_name:\n",
    "            return COMPONENT_COLOURS[comp]\n",
    "    return COMPONENT_COLOURS['minimal']\n",
    "\n",
    "\n",
    "def plot_ablation(results, title, save_path=None, show_delta=False):\n",
    "    \"\"\"Plot horizontal bar chart for ablation results using seaborn.\"\"\"\n",
    "    # Create DataFrame for seaborn\n",
    "    df_plot = pd.DataFrame({\n",
    "        'config': list(results.keys()),\n",
    "        'score': list(results.values())\n",
    "    })\n",
    "    df_plot['color'] = [_get_ablation_color(c) for c in df_plot['config']]\n",
    "    palette = dict(zip(df_plot['config'], df_plot['color']))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.barplot(data=df_plot, y='config', x='score', hue='config', palette=palette, \n",
    "                ax=ax, edgecolor='white', legend=False)\n",
    "    \n",
    "    ax.set_xlabel('Macro F1 Score')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_title(f'{title}\\n(Zero-Shot on Dev Set)')\n",
    "    ax.set_xlim(0, df_plot['score'].max() * 1.15)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (score, config) in enumerate(zip(df_plot['score'], df_plot['config'])):\n",
    "        ax.text(score + 0.005, i, f'{score:.3f}', va='center', fontsize=10)\n",
    "    \n",
    "    # Add baseline reference line (minimal)\n",
    "    if 'minimal' in results:\n",
    "        ax.axvline(x=results['minimal'], color=COMPONENT_COLOURS['minimal'], \n",
    "                   linestyle='--', alpha=0.7, linewidth=1.5, label='Minimal baseline')\n",
    "        ax.legend(loc='lower right')\n",
    "    \n",
    "    # Add delta annotations for cumulative\n",
    "    if show_delta:\n",
    "        scores = list(results.values())\n",
    "        for i in range(1, len(scores)):\n",
    "            delta = scores[i] - scores[i-1]\n",
    "            sign = '+' if delta >= 0 else ''\n",
    "            ax.text(scores[i] + 0.04, i - 0.25, f'{sign}{delta:.3f}', \n",
    "                    fontsize=9, color='dimgray', style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_isolated_ablation(results, save_path=None):\n",
    "    \"\"\"Plot isolated ablation (minimal + ONE component).\"\"\"\n",
    "    return plot_ablation(results, 'System Prompt Ablation: Isolated Component Contribution', \n",
    "                         save_path=save_path, show_delta=False)\n",
    "\n",
    "\n",
    "def plot_cumulative_ablation(results, save_path=None):\n",
    "    \"\"\"Plot cumulative ablation (stacking components).\"\"\"\n",
    "    return plot_ablation(results, 'System Prompt Ablation: Cumulative Component Stacking', \n",
    "                         save_path=save_path, show_delta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddf2d8258e542308600bbb8ea4d9e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (few-shot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Results (few-shot)\n",
      "============================================================\n",
      "Macro F1: 0.4222\n",
      "\n",
      "Per-class F1:\n",
      "  support: 0.4118\n",
      "  deny: 0.3478\n",
      "  query: 0.3680\n",
      "  comment: 0.5612\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     support       0.42      0.41      0.41        69\n",
      "        deny       0.33      0.36      0.35        11\n",
      "       query       0.24      0.82      0.37        28\n",
      "     comment       0.74      0.45      0.56       173\n",
      "\n",
      "    accuracy                           0.47       281\n",
      "   macro avg       0.43      0.51      0.42       281\n",
      "weighted avg       0.60      0.47      0.50       281\n",
      "\n",
      "Saved predictions: ./results/prompting/predictions_few-shot.csv\n"
     ]
    }
   ],
   "source": [
    "# few-shot\n",
    "few_shot_examples = get_few_shot_examples(train_df, n_per_class=1)\n",
    "few_results = evaluate_prompting(model, use_df, mode=\"few-shot\", examples=few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoT Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0da05545f546d4a7575e9406d1b106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating (cot):   0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CoT prompting\n",
    "\n",
    "cot_results = evaluate_prompting(model, use_df, mode=\"cot\", examples=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test a single eg\n",
    "row = dev_df.iloc[0]\n",
    "input_text = format_input_with_context(row, dev_df, use_features=False, use_context=USE_CTX)\n",
    "messages = build_cot_messages(input_text, None)\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "response = model(prompt, COT_REGEX, max_new_tokens=100)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Analyzing the tweet, [TARGET] is providing factual information with a link to a news source, which is a common way to verify the accuracy of a news story. The tone is informative and neutral, without expressing any opinion or bias. Therefore, the reply does not directly support, deny, or query the veracity of the source claim, but rather presents the information as is. Label: COMMENT'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'support'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['label_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Heart goes out to 148 passengers and crew of Germanwings Airbus A320 that has crashed in French Alps, Southern France http://t.co/K7fmJLRt4G'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312k",
   "language": "python",
   "name": "venv312k"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
