# W&B Sweep Configuration for Stance Classifier
# Usage: wandb sweep sweep.yaml
#        wandb agent <sweep_id>
#
# Sweep v3: Final fine-tuning based on sweep v2 results
# - Fixed: attention targets, weighted_ce loss (clear winners)
# - Fine-tuning: lr, lora_r (16-24 sweet spot), warmup, weight_decay

project: NLP_cswk
entity: theo-farrell99-durham-university
method: bayes
metric:
  name: val/macro_f1
  goal: maximize

program: classifier.py
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - "--agent"

parameters:
  model_name:
    value: "bertweet-large"
  max_length:
    values: [256, 512]
  lora_targets:
    values: ["minimal", "attention"]
  learning_rate:
    value: 0.00004
  batch_size:
    value: 8
  num_epochs:
    value: 20
  warmup_ratio:
    values: [0.15, 0.2]  # Fine-tune around 0.15
  weight_decay:
    values: [0.02, 0.03, 0.05]
  lora_r:
    values: [12, 16, 20, 24]
  lora_alpha:
    values: [48, 64, 80]  # Keep ~3-4x ratio to r
  loss_type:
    value: "weighted_ce"
  early_stopping_patience:
    value: 5
