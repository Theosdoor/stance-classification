# W&B Sweep Configuration for Stance Classifier
# Usage: wandb sweep sweep.yaml
#        wandb agent <sweep_id>
#
# Sweep v2: Refined based on previous sweep results
# - Best configs used lr=5e-05, bs=8, lora_r=32, lora_alpha=64, attention targets
# - Now using BERTweet normalization preprocessing
# - Reduced lora_r to avoid OOM on shared GPU

project: NLP_cswk
entity: theo-farrell99-durham-university
method: bayes
metric:
  name: val/macro_f1
  goal: maximize

program: classifier.py
command:
  - ${env}
  - ${interpreter}
  - ${program}
  - "--agent"

parameters:
  model_name:
    value: "bertweet-large"
  lora_targets:
    values: ["attention", "all_linear"]  # Dropped minimal - underperforms
  learning_rate:
    values: [0.00004, 0.00005, 0.00006]  # Narrowed around best (5e-05)
  batch_size:
    value: 8  # Fixed - clear winner over 16
  num_epochs:
    value: 20
  warmup_ratio:
    values: [0.15, 0.2, 0.25]  # Narrowed around best (0.2)
  weight_decay:
    values: [0.01, 0.05, 0.1]  # Dropped 0.0
  lora_r:
    values: [16, 24, 32]  # Reduced from [32, 48, 64] to avoid OOM
  lora_alpha:
    values: [32, 48, 64]  # Reduced from [64, 96, 128] - maintain ~2x ratio to r
